{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('nn_simple_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('nn_simple_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_all = features\n",
    "y_all = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10052, 1280)\n",
      "(10052, 5)\n",
      "(1774, 1280)\n",
      "(1774, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, train_size=.85, random_state=round(time.time()))\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_not_test, y_not_rest, stratify=y_not_rest, train_size=.9, random_state=round(time.time()))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# starter_learning_rate = 0.00\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            50, 0.99, staircase=True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 40 * 32\n",
    "n_classes = 5\n",
    "dropout = .8 # Dropout, probability to keep units\n",
    "\n",
    "# 1. Define Variables and Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "def build_model(x, dropout, activation):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 40, 32, 1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, 4, 5, activation=activation)\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, 8, 3, activation=activation)\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "    conv2 = tf.nn.dropout(conv2, dropout)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "    fc1 = tf.layers.dense(fc1, 128, activation=activation)\n",
    "    fc1 = tf.layers.batch_normalization(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    fc2 = tf.layers.dense(fc1, 64, activation=activation)\n",
    "    fc2 = tf.layers.batch_normalization(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    out = tf.layers.dense(fc2, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "predictions = build_model(x, keep_prob, activation=tf.nn.relu)\n",
    "# 3. Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y), name='loss')\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "\n",
    "# 4. Define the accuracy \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "# 5. Define an optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "def feed_next_batch(train_size, batch_size=64):\n",
    "    \n",
    "    start = 0\n",
    "    while start < train_size:\n",
    "        yield start, start + batch_size\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 10\n",
      "Training loss: 1.62864 , Training acc:  0.22314\n",
      "Test loss: 1.62284 , Test acc:  0.235062\n",
      "--------------------------------------------------------------------------------\n",
      "steps 20\n",
      "Training loss: 1.65521 , Training acc:  0.193096\n",
      "Test loss: 1.65248 , Test acc:  0.193348\n",
      "--------------------------------------------------------------------------------\n",
      "steps 30\n",
      "Training loss: 1.62629 , Training acc:  0.188321\n",
      "Test loss: 1.62553 , Test acc:  0.189966\n",
      "--------------------------------------------------------------------------------\n",
      "steps 40\n",
      "Training loss: 1.61258 , Training acc:  0.199761\n",
      "Test loss: 1.61203 , Test acc:  0.195039\n",
      "--------------------------------------------------------------------------------\n",
      "steps 50\n",
      "Training loss: 1.61191 , Training acc:  0.199463\n",
      "Test loss: 1.61148 , Test acc:  0.200676\n",
      "--------------------------------------------------------------------------------\n",
      "steps 60\n",
      "Training loss: 1.60979 , Training acc:  0.197573\n",
      "Test loss: 1.60932 , Test acc:  0.198985\n",
      "--------------------------------------------------------------------------------\n",
      "steps 70\n",
      "Training loss: 1.61017 , Training acc:  0.199363\n",
      "Test loss: 1.60977 , Test acc:  0.20124\n",
      "--------------------------------------------------------------------------------\n",
      "steps 80\n",
      "Training loss: 1.61005 , Training acc:  0.204636\n",
      "Test loss: 1.60954 , Test acc:  0.200113\n",
      "--------------------------------------------------------------------------------\n",
      "steps 90\n",
      "Training loss: 1.60914 , Training acc:  0.201154\n",
      "Test loss: 1.60851 , Test acc:  0.202931\n",
      "--------------------------------------------------------------------------------\n",
      "steps 100\n",
      "Training loss: 1.60915 , Training acc:  0.212396\n",
      "Test loss: 1.60865 , Test acc:  0.20124\n",
      "--------------------------------------------------------------------------------\n",
      "steps 110\n",
      "Training loss: 1.60955 , Training acc:  0.201353\n",
      "Test loss: 1.60924 , Test acc:  0.202931\n",
      "--------------------------------------------------------------------------------\n",
      "steps 120\n",
      "Training loss: 1.60914 , Training acc:  0.202149\n",
      "Test loss: 1.60863 , Test acc:  0.202931\n",
      "--------------------------------------------------------------------------------\n",
      "steps 130\n",
      "Training loss: 1.60886 , Training acc:  0.202447\n",
      "Test loss: 1.60803 , Test acc:  0.204059\n",
      "--------------------------------------------------------------------------------\n",
      "steps 140\n",
      "Training loss: 1.60873 , Training acc:  0.20394\n",
      "Test loss: 1.60801 , Test acc:  0.204622\n",
      "--------------------------------------------------------------------------------\n",
      "steps 150\n",
      "Training loss: 1.60788 , Training acc:  0.206327\n",
      "Test loss: 1.6075 , Test acc:  0.209132\n",
      "--------------------------------------------------------------------------------\n",
      "steps 160\n",
      "Training loss: 1.60758 , Training acc:  0.211202\n",
      "Test loss: 1.60778 , Test acc:  0.206313\n",
      "--------------------------------------------------------------------------------\n",
      "steps 170\n",
      "Training loss: 1.60622 , Training acc:  0.210903\n",
      "Test loss: 1.60697 , Test acc:  0.206877\n",
      "--------------------------------------------------------------------------------\n",
      "steps 180\n",
      "Training loss: 1.60317 , Training acc:  0.214982\n",
      "Test loss: 1.60494 , Test acc:  0.213641\n",
      "--------------------------------------------------------------------------------\n",
      "steps 190\n",
      "Training loss: 1.60716 , Training acc:  0.212793\n",
      "Test loss: 1.60899 , Test acc:  0.208568\n",
      "--------------------------------------------------------------------------------\n",
      "steps 200\n",
      "Training loss: 1.59158 , Training acc:  0.24035\n",
      "Test loss: 1.59598 , Test acc:  0.235626\n",
      "--------------------------------------------------------------------------------\n",
      "steps 210\n",
      "Training loss: 1.572 , Training acc:  0.270493\n",
      "Test loss: 1.57849 , Test acc:  0.267756\n",
      "--------------------------------------------------------------------------------\n",
      "steps 220\n",
      "Training loss: 1.54923 , Training acc:  0.293474\n",
      "Test loss: 1.55851 , Test acc:  0.291432\n",
      "--------------------------------------------------------------------------------\n",
      "steps 230\n",
      "Training loss: 1.50872 , Training acc:  0.333565\n",
      "Test loss: 1.52068 , Test acc:  0.322435\n",
      "--------------------------------------------------------------------------------\n",
      "steps 240\n",
      "Training loss: 1.50775 , Training acc:  0.333665\n",
      "Test loss: 1.51616 , Test acc:  0.332018\n",
      "--------------------------------------------------------------------------------\n",
      "steps 250\n",
      "Training loss: 1.45291 , Training acc:  0.376045\n",
      "Test loss: 1.46028 , Test acc:  0.371477\n",
      "--------------------------------------------------------------------------------\n",
      "steps 260\n",
      "Training loss: 1.46162 , Training acc:  0.361918\n",
      "Test loss: 1.46535 , Test acc:  0.353439\n",
      "--------------------------------------------------------------------------------\n",
      "steps 270\n",
      "Training loss: 1.47042 , Training acc:  0.355651\n",
      "Test loss: 1.47084 , Test acc:  0.355693\n",
      "--------------------------------------------------------------------------------\n",
      "steps 280\n",
      "Training loss: 1.43515 , Training acc:  0.391265\n",
      "Test loss: 1.44149 , Test acc:  0.393461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 290\n",
      "Training loss: 1.41722 , Training acc:  0.382411\n",
      "Test loss: 1.42793 , Test acc:  0.382751\n",
      "--------------------------------------------------------------------------------\n",
      "steps 300\n",
      "Training loss: 1.37259 , Training acc:  0.405491\n",
      "Test loss: 1.38785 , Test acc:  0.399098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 310\n",
      "Training loss: 1.43801 , Training acc:  0.353462\n",
      "Test loss: 1.44664 , Test acc:  0.351747\n",
      "--------------------------------------------------------------------------------\n",
      "steps 320\n",
      "Training loss: 1.36172 , Training acc:  0.408078\n",
      "Test loss: 1.37207 , Test acc:  0.40699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 330\n",
      "Training loss: 1.49129 , Training acc:  0.330979\n",
      "Test loss: 1.50032 , Test acc:  0.328636\n",
      "--------------------------------------------------------------------------------\n",
      "steps 340\n",
      "Training loss: 1.35139 , Training acc:  0.419519\n",
      "Test loss: 1.37438 , Test acc:  0.400225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 350\n",
      "Training loss: 1.43663 , Training acc:  0.37495\n",
      "Test loss: 1.45055 , Test acc:  0.373732\n",
      "--------------------------------------------------------------------------------\n",
      "steps 360\n",
      "Training loss: 1.34134 , Training acc:  0.433347\n",
      "Test loss: 1.36044 , Test acc:  0.421082\n",
      "--------------------------------------------------------------------------------\n",
      "steps 370\n",
      "Training loss: 1.3372 , Training acc:  0.433247\n",
      "Test loss: 1.35663 , Test acc:  0.425028\n",
      "--------------------------------------------------------------------------------\n",
      "steps 380\n",
      "Training loss: 1.37495 , Training acc:  0.397234\n",
      "Test loss: 1.389 , Test acc:  0.385006\n",
      "--------------------------------------------------------------------------------\n",
      "steps 390\n",
      "Training loss: 1.30599 , Training acc:  0.456327\n",
      "Test loss: 1.32162 , Test acc:  0.457723\n",
      "--------------------------------------------------------------------------------\n",
      "steps 400\n",
      "Training loss: 1.34043 , Training acc:  0.462296\n",
      "Test loss: 1.35465 , Test acc:  0.445321\n",
      "--------------------------------------------------------------------------------\n",
      "steps 410\n",
      "Training loss: 1.30943 , Training acc:  0.452049\n",
      "Test loss: 1.3236 , Test acc:  0.43743\n",
      "--------------------------------------------------------------------------------\n",
      "steps 420\n",
      "Training loss: 1.28767 , Training acc:  0.473637\n",
      "Test loss: 1.30846 , Test acc:  0.457723\n",
      "--------------------------------------------------------------------------------\n",
      "steps 430\n",
      "Training loss: 1.24676 , Training acc:  0.501492\n",
      "Test loss: 1.2701 , Test acc:  0.493799\n",
      "--------------------------------------------------------------------------------\n",
      "steps 440\n",
      "Training loss: 1.25576 , Training acc:  0.510645\n",
      "Test loss: 1.27978 , Test acc:  0.50451\n",
      "--------------------------------------------------------------------------------\n",
      "steps 450\n",
      "Training loss: 1.29681 , Training acc:  0.462495\n",
      "Test loss: 1.30951 , Test acc:  0.445321\n",
      "--------------------------------------------------------------------------------\n",
      "steps 460\n",
      "Training loss: 1.27292 , Training acc:  0.491643\n",
      "Test loss: 1.28733 , Test acc:  0.468433\n",
      "--------------------------------------------------------------------------------\n",
      "steps 470\n",
      "Training loss: 1.1541 , Training acc:  0.556009\n",
      "Test loss: 1.17196 , Test acc:  0.551297\n",
      "--------------------------------------------------------------------------------\n",
      "steps 480\n",
      "Training loss: 1.14982 , Training acc:  0.562674\n",
      "Test loss: 1.16802 , Test acc:  0.546223\n",
      "--------------------------------------------------------------------------------\n",
      "steps 490\n",
      "Training loss: 1.12776 , Training acc:  0.56725\n",
      "Test loss: 1.14368 , Test acc:  0.568207\n",
      "--------------------------------------------------------------------------------\n",
      "steps 500\n",
      "Training loss: 1.09908 , Training acc:  0.589136\n",
      "Test loss: 1.12229 , Test acc:  0.579481\n",
      "--------------------------------------------------------------------------------\n",
      "steps 510\n",
      "Training loss: 1.11758 , Training acc:  0.577497\n",
      "Test loss: 1.13845 , Test acc:  0.569899\n",
      "--------------------------------------------------------------------------------\n",
      "steps 520\n",
      "Training loss: 1.06043 , Training acc:  0.611321\n",
      "Test loss: 1.08753 , Test acc:  0.606539\n",
      "--------------------------------------------------------------------------------\n",
      "steps 530\n",
      "Training loss: 1.0402 , Training acc:  0.613311\n",
      "Test loss: 1.06494 , Test acc:  0.604284\n",
      "--------------------------------------------------------------------------------\n",
      "steps 540\n",
      "Training loss: 1.05709 , Training acc:  0.613709\n",
      "Test loss: 1.08163 , Test acc:  0.609357\n",
      "--------------------------------------------------------------------------------\n",
      "steps 550\n",
      "Training loss: 1.01202 , Training acc:  0.613709\n",
      "Test loss: 1.02915 , Test acc:  0.599775\n",
      "--------------------------------------------------------------------------------\n",
      "steps 560\n",
      "Training loss: 1.00802 , Training acc:  0.626144\n",
      "Test loss: 1.0342 , Test acc:  0.609921\n",
      "--------------------------------------------------------------------------------\n",
      "steps 570\n",
      "Training loss: 0.945669 , Training acc:  0.645046\n",
      "Test loss: 0.969901 , Test acc:  0.638106\n",
      "--------------------------------------------------------------------------------\n",
      "steps 580\n",
      "Training loss: 0.948208 , Training acc:  0.656586\n",
      "Test loss: 0.972063 , Test acc:  0.650507\n",
      "--------------------------------------------------------------------------------\n",
      "steps 590\n",
      "Training loss: 0.904464 , Training acc:  0.67111\n",
      "Test loss: 0.922342 , Test acc:  0.658399\n",
      "--------------------------------------------------------------------------------\n",
      "steps 600\n",
      "Training loss: 0.892326 , Training acc:  0.688818\n",
      "Test loss: 0.9166 , Test acc:  0.677565\n",
      "--------------------------------------------------------------------------------\n",
      "steps 610\n",
      "Training loss: 0.89612 , Training acc:  0.676681\n",
      "Test loss: 0.920533 , Test acc:  0.666291\n",
      "--------------------------------------------------------------------------------\n",
      "steps 620\n",
      "Training loss: 0.826399 , Training acc:  0.709411\n",
      "Test loss: 0.843552 , Test acc:  0.697294\n",
      "--------------------------------------------------------------------------------\n",
      "steps 630\n",
      "Training loss: 0.810389 , Training acc:  0.714783\n",
      "Test loss: 0.824761 , Test acc:  0.702368\n",
      "--------------------------------------------------------------------------------\n",
      "steps 640\n",
      "Training loss: 0.782355 , Training acc:  0.719459\n",
      "Test loss: 0.797824 , Test acc:  0.710823\n",
      "--------------------------------------------------------------------------------\n",
      "steps 650\n",
      "Training loss: 0.790511 , Training acc:  0.723637\n",
      "Test loss: 0.812715 , Test acc:  0.701804\n",
      "--------------------------------------------------------------------------------\n",
      "steps 660\n",
      "Training loss: 0.797194 , Training acc:  0.724333\n",
      "Test loss: 0.818652 , Test acc:  0.700113\n",
      "--------------------------------------------------------------------------------\n",
      "steps 670\n",
      "Training loss: 0.756321 , Training acc:  0.742141\n",
      "Test loss: 0.776097 , Test acc:  0.728298\n",
      "--------------------------------------------------------------------------------\n",
      "steps 680\n",
      "Training loss: 0.747359 , Training acc:  0.736968\n",
      "Test loss: 0.768613 , Test acc:  0.727734\n",
      "--------------------------------------------------------------------------------\n",
      "steps 690\n",
      "Training loss: 0.726335 , Training acc:  0.744628\n",
      "Test loss: 0.744765 , Test acc:  0.745209\n",
      "--------------------------------------------------------------------------------\n",
      "steps 700\n",
      "Training loss: 0.695425 , Training acc:  0.750398\n",
      "Test loss: 0.711145 , Test acc:  0.744645\n",
      "--------------------------------------------------------------------------------\n",
      "steps 710\n",
      "Training loss: 0.736838 , Training acc:  0.728711\n",
      "Test loss: 0.760442 , Test acc:  0.715333\n",
      "--------------------------------------------------------------------------------\n",
      "steps 720\n",
      "Training loss: 0.709475 , Training acc:  0.749702\n",
      "Test loss: 0.72177 , Test acc:  0.745209\n",
      "--------------------------------------------------------------------------------\n",
      "steps 730\n",
      "Training loss: 0.67963 , Training acc:  0.762435\n",
      "Test loss: 0.701036 , Test acc:  0.747463\n",
      "--------------------------------------------------------------------------------\n",
      "steps 740\n",
      "Training loss: 0.644979 , Training acc:  0.768106\n",
      "Test loss: 0.671479 , Test acc:  0.753664\n",
      "--------------------------------------------------------------------------------\n",
      "steps 750\n",
      "Training loss: 0.682071 , Training acc:  0.766415\n",
      "Test loss: 0.699833 , Test acc:  0.752537\n",
      "--------------------------------------------------------------------------------\n",
      "steps 760\n",
      "Training loss: 0.699915 , Training acc:  0.750099\n",
      "Test loss: 0.730998 , Test acc:  0.739008\n",
      "--------------------------------------------------------------------------------\n",
      "steps 770\n",
      "Training loss: 0.658727 , Training acc:  0.776761\n",
      "Test loss: 0.669601 , Test acc:  0.770575\n",
      "--------------------------------------------------------------------------------\n",
      "steps 780\n",
      "Training loss: 0.633644 , Training acc:  0.775965\n",
      "Test loss: 0.654735 , Test acc:  0.761556\n",
      "--------------------------------------------------------------------------------\n",
      "steps 790\n",
      "Training loss: 0.642231 , Training acc:  0.772284\n",
      "Test loss: 0.668013 , Test acc:  0.762683\n",
      "--------------------------------------------------------------------------------\n",
      "steps 800\n",
      "Training loss: 0.62515 , Training acc:  0.784322\n",
      "Test loss: 0.653281 , Test acc:  0.767756\n",
      "--------------------------------------------------------------------------------\n",
      "steps 810\n",
      "Training loss: 0.63257 , Training acc:  0.779447\n",
      "Test loss: 0.654901 , Test acc:  0.763247\n",
      "--------------------------------------------------------------------------------\n",
      "steps 820\n",
      "Training loss: 0.62266 , Training acc:  0.786311\n",
      "Test loss: 0.640276 , Test acc:  0.773957\n",
      "--------------------------------------------------------------------------------\n",
      "steps 830\n",
      "Training loss: 0.596241 , Training acc:  0.798448\n",
      "Test loss: 0.634857 , Test acc:  0.780722\n",
      "--------------------------------------------------------------------------------\n",
      "steps 840\n",
      "Training loss: 0.628353 , Training acc:  0.77696\n",
      "Test loss: 0.662591 , Test acc:  0.761556\n",
      "--------------------------------------------------------------------------------\n",
      "steps 850\n",
      "Training loss: 0.590938 , Training acc:  0.797553\n",
      "Test loss: 0.621224 , Test acc:  0.776212\n",
      "--------------------------------------------------------------------------------\n",
      "steps 860\n",
      "Training loss: 0.571524 , Training acc:  0.798846\n",
      "Test loss: 0.594683 , Test acc:  0.77283\n",
      "--------------------------------------------------------------------------------\n",
      "steps 870\n",
      "Training loss: 0.574616 , Training acc:  0.798448\n",
      "Test loss: 0.599078 , Test acc:  0.789177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 880\n",
      "Training loss: 0.571574 , Training acc:  0.802129\n",
      "Test loss: 0.60022 , Test acc:  0.780158\n",
      "--------------------------------------------------------------------------------\n",
      "steps 890\n",
      "Training loss: 0.572295 , Training acc:  0.796259\n",
      "Test loss: 0.60681 , Test acc:  0.777339\n",
      "--------------------------------------------------------------------------------\n",
      "steps 900\n",
      "Training loss: 0.551015 , Training acc:  0.80571\n",
      "Test loss: 0.580365 , Test acc:  0.784104\n",
      "--------------------------------------------------------------------------------\n",
      "steps 910\n",
      "Training loss: 0.531386 , Training acc:  0.814166\n",
      "Test loss: 0.56072 , Test acc:  0.798196\n",
      "--------------------------------------------------------------------------------\n",
      "steps 920\n",
      "Training loss: 0.533164 , Training acc:  0.811779\n",
      "Test loss: 0.559516 , Test acc:  0.803833\n",
      "--------------------------------------------------------------------------------\n",
      "steps 930\n",
      "Training loss: 0.516747 , Training acc:  0.806506\n",
      "Test loss: 0.542951 , Test acc:  0.797632\n",
      "--------------------------------------------------------------------------------\n",
      "steps 940\n",
      "Training loss: 0.501664 , Training acc:  0.819737\n",
      "Test loss: 0.530611 , Test acc:  0.813416\n",
      "--------------------------------------------------------------------------------\n",
      "steps 950\n",
      "Training loss: 0.536508 , Training acc:  0.815062\n",
      "Test loss: 0.567084 , Test acc:  0.793123\n",
      "--------------------------------------------------------------------------------\n",
      "steps 960\n",
      "Training loss: 0.512433 , Training acc:  0.822423\n",
      "Test loss: 0.554107 , Test acc:  0.794814\n",
      "--------------------------------------------------------------------------------\n",
      "steps 970\n",
      "Training loss: 0.502056 , Training acc:  0.828392\n",
      "Test loss: 0.533105 , Test acc:  0.807215\n",
      "--------------------------------------------------------------------------------\n",
      "steps 980\n",
      "Training loss: 0.481252 , Training acc:  0.840131\n",
      "Test loss: 0.511056 , Test acc:  0.817926\n",
      "--------------------------------------------------------------------------------\n",
      "steps 990\n",
      "Training loss: 0.499999 , Training acc:  0.830581\n",
      "Test loss: 0.525262 , Test acc:  0.811161\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1000\n",
      "Training loss: 0.476066 , Training acc:  0.835555\n",
      "Test loss: 0.505458 , Test acc:  0.818489\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1010\n",
      "Training loss: 0.465232 , Training acc:  0.837843\n",
      "Test loss: 0.492345 , Test acc:  0.821871\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1020\n",
      "Training loss: 0.494532 , Training acc:  0.831576\n",
      "Test loss: 0.524121 , Test acc:  0.811161\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1030\n",
      "Training loss: 0.47851 , Training acc:  0.839435\n",
      "Test loss: 0.506712 , Test acc:  0.814543\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1040\n",
      "Training loss: 0.466256 , Training acc:  0.839733\n",
      "Test loss: 0.508099 , Test acc:  0.821308\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1050\n",
      "Training loss: 0.475271 , Training acc:  0.837346\n",
      "Test loss: 0.510841 , Test acc:  0.815107\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1060\n",
      "Training loss: 0.45986 , Training acc:  0.846001\n",
      "Test loss: 0.492356 , Test acc:  0.822999\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1070\n",
      "Training loss: 0.444757 , Training acc:  0.85585\n",
      "Test loss: 0.480359 , Test acc:  0.823563\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1080\n",
      "Training loss: 0.430294 , Training acc:  0.862515\n",
      "Test loss: 0.465028 , Test acc:  0.841601\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1090\n",
      "Training loss: 0.43749 , Training acc:  0.857043\n",
      "Test loss: 0.483575 , Test acc:  0.831454\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1100\n",
      "Training loss: 0.428502 , Training acc:  0.854258\n",
      "Test loss: 0.473058 , Test acc:  0.830327\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1110\n",
      "Training loss: 0.419943 , Training acc:  0.864803\n",
      "Test loss: 0.461348 , Test acc:  0.840474\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1120\n",
      "Training loss: 0.390927 , Training acc:  0.864902\n",
      "Test loss: 0.425055 , Test acc:  0.844983\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1130\n",
      "Training loss: 0.420509 , Training acc:  0.852766\n",
      "Test loss: 0.456915 , Test acc:  0.833145\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1140\n",
      "Training loss: 0.389782 , Training acc:  0.866395\n",
      "Test loss: 0.427909 , Test acc:  0.844419\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1150\n",
      "Training loss: 0.4259 , Training acc:  0.854059\n",
      "Test loss: 0.478675 , Test acc:  0.825817\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1160\n",
      "Training loss: 0.38857 , Training acc:  0.87306\n",
      "Test loss: 0.437448 , Test acc:  0.847802\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1170\n",
      "Training loss: 0.412377 , Training acc:  0.856645\n",
      "Test loss: 0.458427 , Test acc:  0.832018\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1180\n",
      "Training loss: 0.368781 , Training acc:  0.874652\n",
      "Test loss: 0.410238 , Test acc:  0.844983\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1190\n",
      "Training loss: 0.396345 , Training acc:  0.867688\n",
      "Test loss: 0.439658 , Test acc:  0.83991\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1200\n",
      "Training loss: 0.399768 , Training acc:  0.869379\n",
      "Test loss: 0.450651 , Test acc:  0.844419\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1210\n",
      "Training loss: 0.394293 , Training acc:  0.864803\n",
      "Test loss: 0.44488 , Test acc:  0.832582\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1220\n",
      "Training loss: 0.392093 , Training acc:  0.869379\n",
      "Test loss: 0.450807 , Test acc:  0.836528\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1230\n",
      "Training loss: 0.344351 , Training acc:  0.88649\n",
      "Test loss: 0.399431 , Test acc:  0.857948\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1240\n",
      "Training loss: 0.384597 , Training acc:  0.873558\n",
      "Test loss: 0.439746 , Test acc:  0.84611\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1250\n",
      "Training loss: 0.34806 , Training acc:  0.883207\n",
      "Test loss: 0.413057 , Test acc:  0.851184\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1260\n",
      "Training loss: 0.351963 , Training acc:  0.882511\n",
      "Test loss: 0.410634 , Test acc:  0.847238\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1270\n",
      "Training loss: 0.360105 , Training acc:  0.885098\n",
      "Test loss: 0.411549 , Test acc:  0.856257\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1280\n",
      "Training loss: 0.341108 , Training acc:  0.885296\n",
      "Test loss: 0.394983 , Test acc:  0.862458\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1290\n",
      "Training loss: 0.343458 , Training acc:  0.886391\n",
      "Test loss: 0.387661 , Test acc:  0.865276\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1300\n",
      "Training loss: 0.342116 , Training acc:  0.885495\n",
      "Test loss: 0.400166 , Test acc:  0.861894\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1310\n",
      "Training loss: 0.350434 , Training acc:  0.882213\n",
      "Test loss: 0.397803 , Test acc:  0.857384\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1320\n",
      "Training loss: 0.346252 , Training acc:  0.888977\n",
      "Test loss: 0.399989 , Test acc:  0.864713\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1330\n",
      "Training loss: 0.359033 , Training acc:  0.880024\n",
      "Test loss: 0.412063 , Test acc:  0.848929\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1340\n",
      "Training loss: 0.330439 , Training acc:  0.895941\n",
      "Test loss: 0.385086 , Test acc:  0.866404\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1350\n",
      "Training loss: 0.336762 , Training acc:  0.890171\n",
      "Test loss: 0.391209 , Test acc:  0.866967\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1360\n",
      "Training loss: 0.327053 , Training acc:  0.893951\n",
      "Test loss: 0.377338 , Test acc:  0.867531\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1370\n",
      "Training loss: 0.303101 , Training acc:  0.900517\n",
      "Test loss: 0.354894 , Test acc:  0.870349\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1380\n",
      "Training loss: 0.302277 , Training acc:  0.900617\n",
      "Test loss: 0.372543 , Test acc:  0.871477\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1390\n",
      "Training loss: 0.30868 , Training acc:  0.904994\n",
      "Test loss: 0.369051 , Test acc:  0.877678\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1400\n",
      "Training loss: 0.31664 , Training acc:  0.902905\n",
      "Test loss: 0.379627 , Test acc:  0.873732\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1410\n",
      "Training loss: 0.304603 , Training acc:  0.903701\n",
      "Test loss: 0.362875 , Test acc:  0.877114\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1420\n",
      "Training loss: 0.285808 , Training acc:  0.906387\n",
      "Test loss: 0.353771 , Test acc:  0.882187\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1430\n",
      "Training loss: 0.277742 , Training acc:  0.907979\n",
      "Test loss: 0.350208 , Test acc:  0.878805\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1440\n",
      "Training loss: 0.274731 , Training acc:  0.908476\n",
      "Test loss: 0.343096 , Test acc:  0.88106\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1450\n",
      "Training loss: 0.276344 , Training acc:  0.909968\n",
      "Test loss: 0.34421 , Test acc:  0.874295\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1460\n",
      "Training loss: 0.331851 , Training acc:  0.887883\n",
      "Test loss: 0.403833 , Test acc:  0.854002\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1470\n",
      "Training loss: 0.314676 , Training acc:  0.900219\n",
      "Test loss: 0.374047 , Test acc:  0.870349\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1480\n",
      "Training loss: 0.33491 , Training acc:  0.88261\n",
      "Test loss: 0.404524 , Test acc:  0.861894\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1490\n",
      "Training loss: 0.277383 , Training acc:  0.917031\n",
      "Test loss: 0.352638 , Test acc:  0.884442\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1500\n",
      "Training loss: 0.271815 , Training acc:  0.917927\n",
      "Test loss: 0.343251 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1510\n",
      "Training loss: 0.282069 , Training acc:  0.910267\n",
      "Test loss: 0.350545 , Test acc:  0.871477\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1520\n",
      "Training loss: 0.282021 , Training acc:  0.90579\n",
      "Test loss: 0.357544 , Test acc:  0.873732\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1530\n",
      "Training loss: 0.285275 , Training acc:  0.902805\n",
      "Test loss: 0.357001 , Test acc:  0.868095\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1540\n",
      "Training loss: 0.26176 , Training acc:  0.916335\n",
      "Test loss: 0.338391 , Test acc:  0.878241\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1550\n",
      "Training loss: 0.261941 , Training acc:  0.921906\n",
      "Test loss: 0.32688 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1560\n",
      "Training loss: 0.286355 , Training acc:  0.906188\n",
      "Test loss: 0.344224 , Test acc:  0.884442\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1570\n",
      "Training loss: 0.253555 , Training acc:  0.916832\n",
      "Test loss: 0.319064 , Test acc:  0.886697\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1580\n",
      "Training loss: 0.262871 , Training acc:  0.918126\n",
      "Test loss: 0.340932 , Test acc:  0.883315\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1590\n",
      "Training loss: 0.264402 , Training acc:  0.920016\n",
      "Test loss: 0.343543 , Test acc:  0.881623\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1600\n",
      "Training loss: 0.260158 , Training acc:  0.92111\n",
      "Test loss: 0.349169 , Test acc:  0.879932\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1610\n",
      "Training loss: 0.263882 , Training acc:  0.913947\n",
      "Test loss: 0.347366 , Test acc:  0.87655\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1620\n",
      "Training loss: 0.289004 , Training acc:  0.90191\n",
      "Test loss: 0.369777 , Test acc:  0.877678\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1630\n",
      "Training loss: 0.256226 , Training acc:  0.920314\n",
      "Test loss: 0.33297 , Test acc:  0.894589\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1640\n",
      "Training loss: 0.270846 , Training acc:  0.912157\n",
      "Test loss: 0.34437 , Test acc:  0.883878\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1650\n",
      "Training loss: 0.2462 , Training acc:  0.925189\n",
      "Test loss: 0.334995 , Test acc:  0.877678\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1660\n",
      "Training loss: 0.238494 , Training acc:  0.92111\n",
      "Test loss: 0.3205 , Test acc:  0.889515\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1670\n",
      "Training loss: 0.251278 , Training acc:  0.927477\n",
      "Test loss: 0.339045 , Test acc:  0.889515\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1680\n",
      "Training loss: 0.236692 , Training acc:  0.931954\n",
      "Test loss: 0.320401 , Test acc:  0.892334\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1690\n",
      "Training loss: 0.244298 , Training acc:  0.925288\n",
      "Test loss: 0.325619 , Test acc:  0.883878\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1700\n",
      "Training loss: 0.218167 , Training acc:  0.931158\n",
      "Test loss: 0.30116 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1710\n",
      "Training loss: 0.207337 , Training acc:  0.93454\n",
      "Test loss: 0.304415 , Test acc:  0.890079\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1720\n",
      "Training loss: 0.22066 , Training acc:  0.932451\n",
      "Test loss: 0.320243 , Test acc:  0.885006\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1730\n",
      "Training loss: 0.231248 , Training acc:  0.928472\n",
      "Test loss: 0.333127 , Test acc:  0.879932\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1740\n",
      "Training loss: 0.216844 , Training acc:  0.935137\n",
      "Test loss: 0.304525 , Test acc:  0.887824\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1750\n",
      "Training loss: 0.219044 , Training acc:  0.933844\n",
      "Test loss: 0.310718 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1760\n",
      "Training loss: 0.227839 , Training acc:  0.928173\n",
      "Test loss: 0.326043 , Test acc:  0.88726\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1770\n",
      "Training loss: 0.227673 , Training acc:  0.929865\n",
      "Test loss: 0.321596 , Test acc:  0.888388\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1780\n",
      "Training loss: 0.220454 , Training acc:  0.935436\n",
      "Test loss: 0.310605 , Test acc:  0.894025\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1790\n",
      "Training loss: 0.225456 , Training acc:  0.933148\n",
      "Test loss: 0.302944 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1800\n",
      "Training loss: 0.215018 , Training acc:  0.936132\n",
      "Test loss: 0.299063 , Test acc:  0.894025\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1810\n",
      "Training loss: 0.217814 , Training acc:  0.933048\n",
      "Test loss: 0.299467 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1820\n",
      "Training loss: 0.282917 , Training acc:  0.906685\n",
      "Test loss: 0.378725 , Test acc:  0.870349\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1830\n",
      "Training loss: 0.232757 , Training acc:  0.926781\n",
      "Test loss: 0.320832 , Test acc:  0.886697\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1840\n",
      "Training loss: 0.239853 , Training acc:  0.924692\n",
      "Test loss: 0.324206 , Test acc:  0.888388\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1850\n",
      "Training loss: 0.226832 , Training acc:  0.933148\n",
      "Test loss: 0.318715 , Test acc:  0.892334\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1860\n",
      "Training loss: 0.233095 , Training acc:  0.924095\n",
      "Test loss: 0.33752 , Test acc:  0.883315\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1870\n",
      "Training loss: 0.212057 , Training acc:  0.933247\n",
      "Test loss: 0.307709 , Test acc:  0.893461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1880\n",
      "Training loss: 0.226178 , Training acc:  0.92877\n",
      "Test loss: 0.336388 , Test acc:  0.879932\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1890\n",
      "Training loss: 0.236713 , Training acc:  0.922901\n",
      "Test loss: 0.327822 , Test acc:  0.879932\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1900\n",
      "Training loss: 0.224007 , Training acc:  0.928273\n",
      "Test loss: 0.325376 , Test acc:  0.882751\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1910\n",
      "Training loss: 0.237904 , Training acc:  0.921508\n",
      "Test loss: 0.338585 , Test acc:  0.875986\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1920\n",
      "Training loss: 0.195678 , Training acc:  0.940111\n",
      "Test loss: 0.283479 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1930\n",
      "Training loss: 0.215552 , Training acc:  0.93265\n",
      "Test loss: 0.310335 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1940\n",
      "Training loss: 0.207006 , Training acc:  0.932949\n",
      "Test loss: 0.301657 , Test acc:  0.890079\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1950\n",
      "Training loss: 0.205485 , Training acc:  0.93454\n",
      "Test loss: 0.313714 , Test acc:  0.882187\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1960\n",
      "Training loss: 0.230706 , Training acc:  0.928074\n",
      "Test loss: 0.335219 , Test acc:  0.886697\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1970\n",
      "Training loss: 0.252957 , Training acc:  0.913649\n",
      "Test loss: 0.348224 , Test acc:  0.869786\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1980\n",
      "Training loss: 0.21664 , Training acc:  0.932352\n",
      "Test loss: 0.317109 , Test acc:  0.887824\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1990\n",
      "Training loss: 0.217614 , Training acc:  0.932352\n",
      "Test loss: 0.309637 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2000\n",
      "Training loss: 0.218494 , Training acc:  0.93265\n",
      "Test loss: 0.30697 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2010\n",
      "Training loss: 0.200728 , Training acc:  0.940907\n",
      "Test loss: 0.300135 , Test acc:  0.890643\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2020\n",
      "Training loss: 0.192922 , Training acc:  0.9424\n",
      "Test loss: 0.299115 , Test acc:  0.894589\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2030\n",
      "Training loss: 0.211979 , Training acc:  0.932849\n",
      "Test loss: 0.319973 , Test acc:  0.893461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2040\n",
      "Training loss: 0.215915 , Training acc:  0.932849\n",
      "Test loss: 0.318851 , Test acc:  0.883878\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2050\n",
      "Training loss: 0.201864 , Training acc:  0.930959\n",
      "Test loss: 0.30202 , Test acc:  0.88726\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2060\n",
      "Training loss: 0.194525 , Training acc:  0.935635\n",
      "Test loss: 0.301704 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2070\n",
      "Training loss: 0.193923 , Training acc:  0.938122\n",
      "Test loss: 0.285733 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2080\n",
      "Training loss: 0.194878 , Training acc:  0.942499\n",
      "Test loss: 0.30089 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2090\n",
      "Training loss: 0.214661 , Training acc:  0.932551\n",
      "Test loss: 0.325931 , Test acc:  0.886133\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2100\n",
      "Training loss: 0.189273 , Training acc:  0.94419\n",
      "Test loss: 0.287914 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2110\n",
      "Training loss: 0.212024 , Training acc:  0.932153\n",
      "Test loss: 0.310232 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2120\n",
      "Training loss: 0.207765 , Training acc:  0.934142\n",
      "Test loss: 0.308013 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2130\n",
      "Training loss: 0.227435 , Training acc:  0.926781\n",
      "Test loss: 0.335606 , Test acc:  0.881623\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2140\n",
      "Training loss: 0.188565 , Training acc:  0.939316\n",
      "Test loss: 0.284628 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2150\n",
      "Training loss: 0.18758 , Training acc:  0.936828\n",
      "Test loss: 0.289021 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2160\n",
      "Training loss: 0.176625 , Training acc:  0.945881\n",
      "Test loss: 0.284973 , Test acc:  0.905862\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2170\n",
      "Training loss: 0.171559 , Training acc:  0.949861\n",
      "Test loss: 0.290678 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2180\n",
      "Training loss: 0.17462 , Training acc:  0.946279\n",
      "Test loss: 0.288007 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2190\n",
      "Training loss: 0.174952 , Training acc:  0.94807\n",
      "Test loss: 0.285781 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2200\n",
      "Training loss: 0.181757 , Training acc:  0.947672\n",
      "Test loss: 0.294903 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2210\n",
      "Training loss: 0.180523 , Training acc:  0.943096\n",
      "Test loss: 0.297312 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2220\n",
      "Training loss: 0.186153 , Training acc:  0.941007\n",
      "Test loss: 0.300405 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2230\n",
      "Training loss: 0.172353 , Training acc:  0.949065\n",
      "Test loss: 0.275203 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2240\n",
      "Training loss: 0.188237 , Training acc:  0.942598\n",
      "Test loss: 0.307425 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2250\n",
      "Training loss: 0.170093 , Training acc:  0.945881\n",
      "Test loss: 0.286934 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2260\n",
      "Training loss: 0.186888 , Training acc:  0.9423\n",
      "Test loss: 0.314625 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2270\n",
      "Training loss: 0.199063 , Training acc:  0.931556\n",
      "Test loss: 0.306087 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2280\n",
      "Training loss: 0.179491 , Training acc:  0.948269\n",
      "Test loss: 0.304483 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2290\n",
      "Training loss: 0.182872 , Training acc:  0.946976\n",
      "Test loss: 0.295602 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2300\n",
      "Training loss: 0.181858 , Training acc:  0.945285\n",
      "Test loss: 0.294346 , Test acc:  0.894589\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2310\n",
      "Training loss: 0.180654 , Training acc:  0.94608\n",
      "Test loss: 0.293922 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2320\n",
      "Training loss: 0.188135 , Training acc:  0.944091\n",
      "Test loss: 0.298249 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2330\n",
      "Training loss: 0.207057 , Training acc:  0.938122\n",
      "Test loss: 0.321541 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2340\n",
      "Training loss: 0.16377 , Training acc:  0.952845\n",
      "Test loss: 0.279395 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2350\n",
      "Training loss: 0.163185 , Training acc:  0.94996\n",
      "Test loss: 0.280585 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2360\n",
      "Training loss: 0.169916 , Training acc:  0.946578\n",
      "Test loss: 0.282772 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2370\n",
      "Training loss: 0.165836 , Training acc:  0.951552\n",
      "Test loss: 0.290759 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2380\n",
      "Training loss: 0.178588 , Training acc:  0.942698\n",
      "Test loss: 0.312518 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2390\n",
      "Training loss: 0.1688 , Training acc:  0.949264\n",
      "Test loss: 0.302556 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2400\n",
      "Training loss: 0.161369 , Training acc:  0.94996\n",
      "Test loss: 0.288284 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2410\n",
      "Training loss: 0.148397 , Training acc:  0.952945\n",
      "Test loss: 0.267829 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2420\n",
      "Training loss: 0.161329 , Training acc:  0.949662\n",
      "Test loss: 0.283143 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2430\n",
      "Training loss: 0.167799 , Training acc:  0.946976\n",
      "Test loss: 0.29238 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2440\n",
      "Training loss: 0.158828 , Training acc:  0.948766\n",
      "Test loss: 0.283285 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2450\n",
      "Training loss: 0.137732 , Training acc:  0.959511\n",
      "Test loss: 0.264123 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2460\n",
      "Training loss: 0.147934 , Training acc:  0.954039\n",
      "Test loss: 0.279175 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2470\n",
      "Training loss: 0.150873 , Training acc:  0.952845\n",
      "Test loss: 0.281909 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2480\n",
      "Training loss: 0.155263 , Training acc:  0.950458\n",
      "Test loss: 0.280167 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2490\n",
      "Training loss: 0.166746 , Training acc:  0.949065\n",
      "Test loss: 0.286073 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2500\n",
      "Training loss: 0.151682 , Training acc:  0.956128\n",
      "Test loss: 0.271688 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2510\n",
      "Training loss: 0.154848 , Training acc:  0.957024\n",
      "Test loss: 0.285332 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2520\n",
      "Training loss: 0.159829 , Training acc:  0.953939\n",
      "Test loss: 0.288212 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2530\n",
      "Training loss: 0.149783 , Training acc:  0.954835\n",
      "Test loss: 0.281414 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2540\n",
      "Training loss: 0.136027 , Training acc:  0.960207\n",
      "Test loss: 0.266955 , Test acc:  0.910936\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2550\n",
      "Training loss: 0.14608 , Training acc:  0.95583\n",
      "Test loss: 0.28521 , Test acc:  0.894589\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2560\n",
      "Training loss: 0.150673 , Training acc:  0.954934\n",
      "Test loss: 0.277812 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2570\n",
      "Training loss: 0.15615 , Training acc:  0.949562\n",
      "Test loss: 0.292155 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2580\n",
      "Training loss: 0.157718 , Training acc:  0.951154\n",
      "Test loss: 0.300256 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2590\n",
      "Training loss: 0.139208 , Training acc:  0.95772\n",
      "Test loss: 0.288282 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2600\n",
      "Training loss: 0.167389 , Training acc:  0.94608\n",
      "Test loss: 0.323111 , Test acc:  0.889515\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2610\n",
      "Training loss: 0.172306 , Training acc:  0.952945\n",
      "Test loss: 0.301604 , Test acc:  0.905862\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2620\n",
      "Training loss: 0.161172 , Training acc:  0.954835\n",
      "Test loss: 0.296323 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2630\n",
      "Training loss: 0.140484 , Training acc:  0.960306\n",
      "Test loss: 0.279913 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2640\n",
      "Training loss: 0.14464 , Training acc:  0.956924\n",
      "Test loss: 0.285167 , Test acc:  0.89628\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2650\n",
      "Training loss: 0.140459 , Training acc:  0.959212\n",
      "Test loss: 0.275245 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2660\n",
      "Training loss: 0.128756 , Training acc:  0.960306\n",
      "Test loss: 0.270691 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2670\n",
      "Training loss: 0.138528 , Training acc:  0.95384\n",
      "Test loss: 0.287291 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2680\n",
      "Training loss: 0.131637 , Training acc:  0.959908\n",
      "Test loss: 0.277688 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2690\n",
      "Training loss: 0.136241 , Training acc:  0.95772\n",
      "Test loss: 0.271144 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2700\n",
      "Training loss: 0.135624 , Training acc:  0.961401\n",
      "Test loss: 0.269755 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2710\n",
      "Training loss: 0.138859 , Training acc:  0.960605\n",
      "Test loss: 0.282898 , Test acc:  0.905862\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2720\n",
      "Training loss: 0.142846 , Training acc:  0.959212\n",
      "Test loss: 0.285772 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2730\n",
      "Training loss: 0.162517 , Training acc:  0.94996\n",
      "Test loss: 0.321896 , Test acc:  0.877678\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2740\n",
      "Training loss: 0.131644 , Training acc:  0.962595\n",
      "Test loss: 0.269857 , Test acc:  0.905299\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2750\n",
      "Training loss: 0.146196 , Training acc:  0.957222\n",
      "Test loss: 0.276233 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2760\n",
      "Training loss: 0.14797 , Training acc:  0.958814\n",
      "Test loss: 0.276342 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2770\n",
      "Training loss: 0.128777 , Training acc:  0.960505\n",
      "Test loss: 0.280694 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2780\n",
      "Training loss: 0.127139 , Training acc:  0.962396\n",
      "Test loss: 0.276328 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2790\n",
      "Training loss: 0.130648 , Training acc:  0.961998\n",
      "Test loss: 0.271336 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2800\n",
      "Training loss: 0.146404 , Training acc:  0.959411\n",
      "Test loss: 0.281315 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2810\n",
      "Training loss: 0.140567 , Training acc:  0.9615\n",
      "Test loss: 0.279043 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2820\n",
      "Training loss: 0.126776 , Training acc:  0.964186\n",
      "Test loss: 0.263433 , Test acc:  0.910372\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2830\n",
      "Training loss: 0.138543 , Training acc:  0.962097\n",
      "Test loss: 0.283763 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2840\n",
      "Training loss: 0.129205 , Training acc:  0.962694\n",
      "Test loss: 0.272231 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2850\n",
      "Training loss: 0.130199 , Training acc:  0.962296\n",
      "Test loss: 0.282164 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2860\n",
      "Training loss: 0.120377 , Training acc:  0.962097\n",
      "Test loss: 0.269009 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2870\n",
      "Training loss: 0.116707 , Training acc:  0.965181\n",
      "Test loss: 0.272516 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2880\n",
      "Training loss: 0.119651 , Training acc:  0.964982\n",
      "Test loss: 0.27678 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2890\n",
      "Training loss: 0.123466 , Training acc:  0.964783\n",
      "Test loss: 0.268301 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2900\n",
      "Training loss: 0.137491 , Training acc:  0.958217\n",
      "Test loss: 0.280498 , Test acc:  0.895716\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2910\n",
      "Training loss: 0.133645 , Training acc:  0.9615\n",
      "Test loss: 0.276893 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2920\n",
      "Training loss: 0.127933 , Training acc:  0.964385\n",
      "Test loss: 0.277301 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2930\n",
      "Training loss: 0.161349 , Training acc:  0.947772\n",
      "Test loss: 0.317138 , Test acc:  0.890643\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2940\n",
      "Training loss: 0.132432 , Training acc:  0.962197\n",
      "Test loss: 0.285281 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2950\n",
      "Training loss: 0.13906 , Training acc:  0.957919\n",
      "Test loss: 0.289103 , Test acc:  0.892897\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2960\n",
      "Training loss: 0.140057 , Training acc:  0.958615\n",
      "Test loss: 0.28781 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2970\n",
      "Training loss: 0.151687 , Training acc:  0.955531\n",
      "Test loss: 0.296657 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2980\n",
      "Training loss: 0.130711 , Training acc:  0.968066\n",
      "Test loss: 0.272319 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2990\n",
      "Training loss: 0.125393 , Training acc:  0.964883\n",
      "Test loss: 0.270142 , Test acc:  0.905299\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3000\n",
      "Training loss: 0.126773 , Training acc:  0.962396\n",
      "Test loss: 0.27769 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3010\n",
      "Training loss: 0.123976 , Training acc:  0.963788\n",
      "Test loss: 0.28179 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3020\n",
      "Training loss: 0.135063 , Training acc:  0.95971\n",
      "Test loss: 0.287623 , Test acc:  0.894025\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3030\n",
      "Training loss: 0.118392 , Training acc:  0.96727\n",
      "Test loss: 0.271274 , Test acc:  0.901353\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3040\n",
      "Training loss: 0.122858 , Training acc:  0.962694\n",
      "Test loss: 0.277402 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3050\n",
      "Training loss: 0.113592 , Training acc:  0.968663\n",
      "Test loss: 0.270706 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3060\n",
      "Training loss: 0.108373 , Training acc:  0.970852\n",
      "Test loss: 0.264806 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3070\n",
      "Training loss: 0.110743 , Training acc:  0.967569\n",
      "Test loss: 0.26356 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3080\n",
      "Training loss: 0.111462 , Training acc:  0.966574\n",
      "Test loss: 0.261801 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3090\n",
      "Training loss: 0.121287 , Training acc:  0.966076\n",
      "Test loss: 0.279063 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3100\n",
      "Training loss: 0.109849 , Training acc:  0.968961\n",
      "Test loss: 0.257395 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3110\n",
      "Training loss: 0.108548 , Training acc:  0.96926\n",
      "Test loss: 0.274857 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3120\n",
      "Training loss: 0.108953 , Training acc:  0.968364\n",
      "Test loss: 0.279624 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3130\n",
      "Training loss: 0.10664 , Training acc:  0.971647\n",
      "Test loss: 0.266857 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3140\n",
      "Training loss: 0.140067 , Training acc:  0.958516\n",
      "Test loss: 0.307898 , Test acc:  0.888952\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3150\n",
      "Training loss: 0.126557 , Training acc:  0.961699\n",
      "Test loss: 0.282323 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3160\n",
      "Training loss: 0.123966 , Training acc:  0.96339\n",
      "Test loss: 0.278184 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3170\n",
      "Training loss: 0.117665 , Training acc:  0.96538\n",
      "Test loss: 0.283359 , Test acc:  0.901353\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3180\n",
      "Training loss: 0.117542 , Training acc:  0.96339\n",
      "Test loss: 0.271232 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3190\n",
      "Training loss: 0.120866 , Training acc:  0.96349\n",
      "Test loss: 0.272428 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3200\n",
      "Training loss: 0.115908 , Training acc:  0.96727\n",
      "Test loss: 0.268936 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3210\n",
      "Training loss: 0.120649 , Training acc:  0.967867\n",
      "Test loss: 0.275311 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3220\n",
      "Training loss: 0.116913 , Training acc:  0.966673\n",
      "Test loss: 0.269499 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3230\n",
      "Training loss: 0.120685 , Training acc:  0.966574\n",
      "Test loss: 0.26914 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3240\n",
      "Training loss: 0.12062 , Training acc:  0.966474\n",
      "Test loss: 0.26865 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3250\n",
      "Training loss: 0.123148 , Training acc:  0.964087\n",
      "Test loss: 0.286374 , Test acc:  0.892897\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3260\n",
      "Training loss: 0.108207 , Training acc:  0.969857\n",
      "Test loss: 0.273521 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3270\n",
      "Training loss: 0.117526 , Training acc:  0.967071\n",
      "Test loss: 0.283997 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3280\n",
      "Training loss: 0.116386 , Training acc:  0.96538\n",
      "Test loss: 0.276643 , Test acc:  0.905299\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3290\n",
      "Training loss: 0.117213 , Training acc:  0.966972\n",
      "Test loss: 0.270345 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3300\n",
      "Training loss: 0.110381 , Training acc:  0.966275\n",
      "Test loss: 0.261269 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3310\n",
      "Training loss: 0.118298 , Training acc:  0.964883\n",
      "Test loss: 0.282131 , Test acc:  0.893461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3320\n",
      "Training loss: 0.111167 , Training acc:  0.972941\n",
      "Test loss: 0.255814 , Test acc:  0.917136\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3330\n",
      "Training loss: 0.119264 , Training acc:  0.970255\n",
      "Test loss: 0.257909 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3340\n",
      "Training loss: 0.11746 , Training acc:  0.96916\n",
      "Test loss: 0.27396 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3350\n",
      "Training loss: 0.101952 , Training acc:  0.973239\n",
      "Test loss: 0.264616 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3360\n",
      "Training loss: 0.0956017 , Training acc:  0.971349\n",
      "Test loss: 0.265897 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3370\n",
      "Training loss: 0.0978776 , Training acc:  0.96926\n",
      "Test loss: 0.269159 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3380\n",
      "Training loss: 0.0997166 , Training acc:  0.969757\n",
      "Test loss: 0.281704 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3390\n",
      "Training loss: 0.0924933 , Training acc:  0.973836\n",
      "Test loss: 0.253657 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3400\n",
      "Training loss: 0.107321 , Training acc:  0.969459\n",
      "Test loss: 0.276572 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3410\n",
      "Training loss: 0.09309 , Training acc:  0.975129\n",
      "Test loss: 0.26209 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3420\n",
      "Training loss: 0.130396 , Training acc:  0.95971\n",
      "Test loss: 0.30997 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3430\n",
      "Training loss: 0.0967864 , Training acc:  0.975726\n",
      "Test loss: 0.265561 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3440\n",
      "Training loss: 0.110266 , Training acc:  0.96916\n",
      "Test loss: 0.270241 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3450\n",
      "Training loss: 0.129424 , Training acc:  0.966275\n",
      "Test loss: 0.301191 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3460\n",
      "Training loss: 0.105155 , Training acc:  0.975129\n",
      "Test loss: 0.269282 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3470\n",
      "Training loss: 0.0929517 , Training acc:  0.974831\n",
      "Test loss: 0.26125 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3480\n",
      "Training loss: 0.104244 , Training acc:  0.970752\n",
      "Test loss: 0.283613 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3490\n",
      "Training loss: 0.101302 , Training acc:  0.973339\n",
      "Test loss: 0.271095 , Test acc:  0.905299\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3500\n",
      "Training loss: 0.0975753 , Training acc:  0.973836\n",
      "Test loss: 0.262792 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3510\n",
      "Training loss: 0.117715 , Training acc:  0.964982\n",
      "Test loss: 0.290317 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3520\n",
      "Training loss: 0.100031 , Training acc:  0.970454\n",
      "Test loss: 0.2684 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3530\n",
      "Training loss: 0.0940654 , Training acc:  0.973637\n",
      "Test loss: 0.267788 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3540\n",
      "Training loss: 0.100783 , Training acc:  0.971548\n",
      "Test loss: 0.282154 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3550\n",
      "Training loss: 0.0932496 , Training acc:  0.975229\n",
      "Test loss: 0.264482 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3560\n",
      "Training loss: 0.0980701 , Training acc:  0.974035\n",
      "Test loss: 0.263726 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3570\n",
      "Training loss: 0.103647 , Training acc:  0.97503\n",
      "Test loss: 0.272297 , Test acc:  0.910936\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3580\n",
      "Training loss: 0.106158 , Training acc:  0.973637\n",
      "Test loss: 0.278472 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3590\n",
      "Training loss: 0.0908186 , Training acc:  0.976323\n",
      "Test loss: 0.268465 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3600\n",
      "Training loss: 0.0913153 , Training acc:  0.978213\n",
      "Test loss: 0.272894 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3610\n",
      "Training loss: 0.0927128 , Training acc:  0.975627\n",
      "Test loss: 0.272975 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3620\n",
      "Training loss: 0.0939461 , Training acc:  0.973239\n",
      "Test loss: 0.269389 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3630\n",
      "Training loss: 0.0953861 , Training acc:  0.975726\n",
      "Test loss: 0.260412 , Test acc:  0.910936\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3640\n",
      "Training loss: 0.0998915 , Training acc:  0.97125\n",
      "Test loss: 0.275836 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3650\n",
      "Training loss: 0.092222 , Training acc:  0.975428\n",
      "Test loss: 0.260429 , Test acc:  0.9177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3660\n",
      "Training loss: 0.0990589 , Training acc:  0.973637\n",
      "Test loss: 0.278792 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3670\n",
      "Training loss: 0.0954585 , Training acc:  0.974234\n",
      "Test loss: 0.272515 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3680\n",
      "Training loss: 0.0938728 , Training acc:  0.972941\n",
      "Test loss: 0.271214 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3690\n",
      "Training loss: 0.094971 , Training acc:  0.972244\n",
      "Test loss: 0.284807 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3700\n",
      "Training loss: 0.0922253 , Training acc:  0.972841\n",
      "Test loss: 0.273838 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3710\n",
      "Training loss: 0.0935395 , Training acc:  0.975229\n",
      "Test loss: 0.267117 , Test acc:  0.915445\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3720\n",
      "Training loss: 0.106429 , Training acc:  0.967569\n",
      "Test loss: 0.287329 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3730\n",
      "Training loss: 0.0887607 , Training acc:  0.974333\n",
      "Test loss: 0.251991 , Test acc:  0.920519\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3740\n",
      "Training loss: 0.10019 , Training acc:  0.973239\n",
      "Test loss: 0.268408 , Test acc:  0.918828\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3750\n",
      "Training loss: 0.101054 , Training acc:  0.975328\n",
      "Test loss: 0.255996 , Test acc:  0.917136\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3760\n",
      "Training loss: 0.102521 , Training acc:  0.974831\n",
      "Test loss: 0.265855 , Test acc:  0.906426\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3770\n",
      "Training loss: 0.1049 , Training acc:  0.970155\n",
      "Test loss: 0.288468 , Test acc:  0.90248\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3780\n",
      "Training loss: 0.0903011 , Training acc:  0.973836\n",
      "Test loss: 0.274989 , Test acc:  0.905299\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3790\n",
      "Training loss: 0.0919232 , Training acc:  0.973438\n",
      "Test loss: 0.271764 , Test acc:  0.910372\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3800\n",
      "Training loss: 0.0953649 , Training acc:  0.973339\n",
      "Test loss: 0.271638 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3810\n",
      "Training loss: 0.0908089 , Training acc:  0.976124\n",
      "Test loss: 0.262696 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3820\n",
      "Training loss: 0.108507 , Training acc:  0.966076\n",
      "Test loss: 0.288107 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3830\n",
      "Training loss: 0.0886396 , Training acc:  0.974731\n",
      "Test loss: 0.277639 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3840\n",
      "Training loss: 0.0848768 , Training acc:  0.978611\n",
      "Test loss: 0.263351 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3850\n",
      "Training loss: 0.0873423 , Training acc:  0.978412\n",
      "Test loss: 0.27276 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3860\n",
      "Training loss: 0.0793136 , Training acc:  0.978313\n",
      "Test loss: 0.266125 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3870\n",
      "Training loss: 0.0967003 , Training acc:  0.969658\n",
      "Test loss: 0.280771 , Test acc:  0.905862\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3880\n",
      "Training loss: 0.125732 , Training acc:  0.958217\n",
      "Test loss: 0.299755 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3890\n",
      "Training loss: 0.0883929 , Training acc:  0.975229\n",
      "Test loss: 0.259858 , Test acc:  0.911499\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3900\n",
      "Training loss: 0.0855515 , Training acc:  0.977915\n",
      "Test loss: 0.269553 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3910\n",
      "Training loss: 0.0926672 , Training acc:  0.974234\n",
      "Test loss: 0.281288 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3920\n",
      "Training loss: 0.0994864 , Training acc:  0.972443\n",
      "Test loss: 0.287133 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3930\n",
      "Training loss: 0.0925608 , Training acc:  0.976224\n",
      "Test loss: 0.261461 , Test acc:  0.918828\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3940\n",
      "Training loss: 0.0909466 , Training acc:  0.975527\n",
      "Test loss: 0.269862 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3950\n",
      "Training loss: 0.0839399 , Training acc:  0.976323\n",
      "Test loss: 0.275667 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3960\n",
      "Training loss: 0.0798377 , Training acc:  0.978114\n",
      "Test loss: 0.266241 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3970\n",
      "Training loss: 0.0787635 , Training acc:  0.9807\n",
      "Test loss: 0.25927 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3980\n",
      "Training loss: 0.0898584 , Training acc:  0.974035\n",
      "Test loss: 0.271971 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3990\n",
      "Training loss: 0.108145 , Training acc:  0.969061\n",
      "Test loss: 0.293187 , Test acc:  0.896843\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4000\n",
      "Training loss: 0.0908639 , Training acc:  0.976323\n",
      "Test loss: 0.249832 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4010\n",
      "Training loss: 0.105545 , Training acc:  0.974333\n",
      "Test loss: 0.252637 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4020\n",
      "Training loss: 0.105995 , Training acc:  0.97304\n",
      "Test loss: 0.2743 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4030\n",
      "Training loss: 0.0982824 , Training acc:  0.975229\n",
      "Test loss: 0.264922 , Test acc:  0.915445\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4040\n",
      "Training loss: 0.0772622 , Training acc:  0.981695\n",
      "Test loss: 0.228552 , Test acc:  0.925028\n",
      "--------------------------------------------------------------------------------\n",
      "Optimization Finished!\n",
      "Training Accuracy: 0.981695\n",
      "Test Accuracy: 0.925028\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer_train = tf.summary.FileWriter('./log/' + 'run_2/train', graph=sess.graph)\n",
    "writer_test = tf.summary.FileWriter('./log/' + 'run_2/test', graph=sess.graph)\n",
    "\n",
    "steps = 0\n",
    "# Keep training until reach max iterations\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train =  X_train[indices], y_train[indices]\n",
    "\n",
    "    for start, end in feed_next_batch(len(X_train), batch_size=batch_size):\n",
    "        # Run optimization op (backprop)\n",
    "        batch_x, batch_y = X_train[start:end], y_train[start:end]\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            train_loss, train_acc, summary_train = sess.run([loss, accuracy, summary], \n",
    "                                                   feed_dict={x: X_train, y: y_train, keep_prob: 1.0})\n",
    "            writer_train.add_summary(summary_train, steps)\n",
    "\n",
    "            print('steps %d' % (steps ))\n",
    "            if type(learning_rate) is not float:\n",
    "                print('learning_rate:', sess.run(learning_rate))\n",
    "\n",
    "            print (\"Training loss:\", train_loss, ', Training acc: ', train_acc)\n",
    "\n",
    "            val_loss, val_acc, summary_test = sess.run([loss, accuracy, summary], feed_dict={x: X_test, \n",
    "                                                                        y: y_test,\n",
    "                                                                       keep_prob: 1.0})\n",
    "\n",
    "            writer_test.add_summary(summary_test, steps)\n",
    "            print (\"Test loss:\", val_loss, ', Test acc: ', val_acc)\n",
    "            print('-' * 80)\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for all test samples\n",
    "print (\"Training Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_train,\n",
    "                                  y: y_train,\n",
    "                                 keep_prob: 1.0}))\n",
    "print (\"Test Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_test,\n",
    "                                  y: y_test,\n",
    "                                 keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
