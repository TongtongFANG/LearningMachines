{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('nn_simple_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('nn_simple_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_all = features\n",
    "y_all = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10052, 1280)\n",
      "(10052, 5)\n",
      "(1774, 1280)\n",
      "(1774, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, train_size=.85, random_state=round(time.time()))\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_not_test, y_not_rest, stratify=y_not_rest, train_size=.9, random_state=round(time.time()))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# learning_rate = 0.005\n",
    "\n",
    "with tf.name_scope(\"learning_rate\"):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.005\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               75, 0.99, staircase=True)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 40 * 32\n",
    "n_classes = 5\n",
    "dropout = .8 # Dropout, probability to keep units\n",
    "\n",
    "# 1. Define Variables and Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "def build_model(x, dropout, activation):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 40, 32, 1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, 4, 5, activation=activation)\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, 8, 3, activation=activation)\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "    conv2 = tf.nn.dropout(conv2, dropout)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "    fc1 = tf.layers.dense(fc1, 128, activation=activation)\n",
    "    fc1 = tf.layers.batch_normalization(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    fc2 = tf.layers.dense(fc1, 64, activation=activation)\n",
    "    fc2 = tf.layers.batch_normalization(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    out = tf.layers.dense(fc2, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "predictions = build_model(x, keep_prob, activation=tf.nn.relu)\n",
    "# 3. Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y), name='loss')\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "\n",
    "# 4. Define the accuracy \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "# 5. Define an optimizer\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "def feed_next_batch(train_size, batch_size=64):\n",
    "    \n",
    "    start = 0\n",
    "    while start < train_size:\n",
    "        yield start, start + batch_size\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "learning_rate: 0.00480298\n",
      "Training loss: 1.40081 , Training acc:  0.397135\n",
      "Test loss: 1.40481 , Test acc:  0.40699\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2\n",
      "learning_rate: 0.00461372\n",
      "Training loss: 1.09444 , Training acc:  0.56148\n",
      "Test loss: 1.11654 , Test acc:  0.553551\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3\n",
      "learning_rate: 0.00443192\n",
      "Training loss: 0.803566 , Training acc:  0.695782\n",
      "Test loss: 0.843929 , Test acc:  0.665727\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4\n",
      "learning_rate: 0.00425729\n",
      "Training loss: 0.703142 , Training acc:  0.739057\n",
      "Test loss: 0.756601 , Test acc:  0.713641\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5\n",
      "learning_rate: 0.00404864\n",
      "Training loss: 0.539761 , Training acc:  0.812774\n",
      "Test loss: 0.588979 , Test acc:  0.78805\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6\n",
      "learning_rate: 0.00388911\n",
      "Training loss: 0.439376 , Training acc:  0.844708\n",
      "Test loss: 0.508858 , Test acc:  0.801578\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7\n",
      "learning_rate: 0.00373586\n",
      "Training loss: 0.421613 , Training acc:  0.856645\n",
      "Test loss: 0.497039 , Test acc:  0.826381\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8\n",
      "learning_rate: 0.00358865\n",
      "Training loss: 0.355247 , Training acc:  0.880322\n",
      "Test loss: 0.447304 , Test acc:  0.833145\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9\n",
      "learning_rate: 0.00344725\n",
      "Training loss: 0.357656 , Training acc:  0.875746\n",
      "Test loss: 0.454175 , Test acc:  0.835964\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10\n",
      "learning_rate: 0.0032783\n",
      "Training loss: 0.305758 , Training acc:  0.89614\n",
      "Test loss: 0.404515 , Test acc:  0.857384\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11\n",
      "learning_rate: 0.00314912\n",
      "Training loss: 0.233781 , Training acc:  0.928273\n",
      "Test loss: 0.335867 , Test acc:  0.870913\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12\n",
      "learning_rate: 0.00302503\n",
      "Training loss: 0.264384 , Training acc:  0.918822\n",
      "Test loss: 0.371179 , Test acc:  0.866967\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13\n",
      "learning_rate: 0.00290583\n",
      "Training loss: 0.20912 , Training acc:  0.933944\n",
      "Test loss: 0.330788 , Test acc:  0.882187\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14\n",
      "learning_rate: 0.00279133\n",
      "Training loss: 0.202406 , Training acc:  0.936729\n",
      "Test loss: 0.333632 , Test acc:  0.877114\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15\n",
      "learning_rate: 0.00265453\n",
      "Training loss: 0.179607 , Training acc:  0.944091\n",
      "Test loss: 0.310689 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16\n",
      "learning_rate: 0.00254993\n",
      "Training loss: 0.167508 , Training acc:  0.952547\n",
      "Test loss: 0.30031 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17\n",
      "learning_rate: 0.00244945\n",
      "Training loss: 0.154479 , Training acc:  0.952049\n",
      "Test loss: 0.301101 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18\n",
      "learning_rate: 0.00235293\n",
      "Training loss: 0.142411 , Training acc:  0.959312\n",
      "Test loss: 0.297348 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19\n",
      "learning_rate: 0.00226022\n",
      "Training loss: 0.122638 , Training acc:  0.962793\n",
      "Test loss: 0.288505 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20\n",
      "learning_rate: 0.00214945\n",
      "Training loss: 0.132451 , Training acc:  0.95971\n",
      "Test loss: 0.289801 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 21\n",
      "learning_rate: 0.00206475\n",
      "Training loss: 0.117958 , Training acc:  0.965181\n",
      "Test loss: 0.286553 , Test acc:  0.901917\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 22\n",
      "learning_rate: 0.00198339\n",
      "Training loss: 0.0947284 , Training acc:  0.97493\n",
      "Test loss: 0.265024 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 23\n",
      "learning_rate: 0.00190524\n",
      "Training loss: 0.0936036 , Training acc:  0.974532\n",
      "Test loss: 0.261568 , Test acc:  0.9177\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 24\n",
      "learning_rate: 0.00183016\n",
      "Training loss: 0.0825108 , Training acc:  0.978711\n",
      "Test loss: 0.263555 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25\n",
      "learning_rate: 0.00174047\n",
      "Training loss: 0.0816648 , Training acc:  0.979805\n",
      "Test loss: 0.264815 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 26\n",
      "learning_rate: 0.00167189\n",
      "Training loss: 0.0861325 , Training acc:  0.977815\n",
      "Test loss: 0.270991 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 27\n",
      "learning_rate: 0.00160601\n",
      "Training loss: 0.0741437 , Training acc:  0.982392\n",
      "Test loss: 0.25196 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 28\n",
      "learning_rate: 0.00154272\n",
      "Training loss: 0.0710468 , Training acc:  0.982988\n",
      "Test loss: 0.259593 , Test acc:  0.919955\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 29\n",
      "learning_rate: 0.00148193\n",
      "Training loss: 0.0678675 , Training acc:  0.983983\n",
      "Test loss: 0.259907 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30\n",
      "learning_rate: 0.00140931\n",
      "Training loss: 0.0655528 , Training acc:  0.985575\n",
      "Test loss: 0.257103 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 31\n",
      "learning_rate: 0.00135377\n",
      "Training loss: 0.0772157 , Training acc:  0.97881\n",
      "Test loss: 0.280655 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 32\n",
      "learning_rate: 0.00130043\n",
      "Training loss: 0.0572514 , Training acc:  0.986968\n",
      "Test loss: 0.269656 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 33\n",
      "learning_rate: 0.00124919\n",
      "Training loss: 0.0617327 , Training acc:  0.983287\n",
      "Test loss: 0.264988 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 34\n",
      "learning_rate: 0.00119996\n",
      "Training loss: 0.0505475 , Training acc:  0.989753\n",
      "Test loss: 0.267146 , Test acc:  0.919391\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35\n",
      "learning_rate: 0.00114115\n",
      "Training loss: 0.0536398 , Training acc:  0.988758\n",
      "Test loss: 0.258712 , Test acc:  0.918828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 36\n",
      "learning_rate: 0.00109619\n",
      "Training loss: 0.0520089 , Training acc:  0.987366\n",
      "Test loss: 0.262472 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 37\n",
      "learning_rate: 0.00105299\n",
      "Training loss: 0.0454507 , Training acc:  0.991444\n",
      "Test loss: 0.259432 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 38\n",
      "learning_rate: 0.0010115\n",
      "Training loss: 0.0418485 , Training acc:  0.992041\n",
      "Test loss: 0.266795 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 39\n",
      "learning_rate: 0.000971644\n",
      "Training loss: 0.0429763 , Training acc:  0.99035\n",
      "Test loss: 0.260691 , Test acc:  0.918264\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 40\n",
      "learning_rate: 0.000924024\n",
      "Training loss: 0.0389268 , Training acc:  0.992141\n",
      "Test loss: 0.269925 , Test acc:  0.915445\n",
      "--------------------------------------------------------------------------------\n",
      "Optimization Finished!\n",
      "Training Accuracy: 0.992141\n",
      "Test Accuracy: 0.915445\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "RUN_NAME = 'batch_size_32_0.005_starting_lr'\n",
    "writer_train = tf.summary.FileWriter('./log/' + RUN_NAME + '/train', graph=sess.graph)\n",
    "writer_test = tf.summary.FileWriter('./log/' + RUN_NAME + '/test', graph=sess.graph)\n",
    "\n",
    "steps = 0\n",
    "# Keep training until reach max iterations\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train =  X_train[indices], y_train[indices]\n",
    "\n",
    "    for start, end in feed_next_batch(len(X_train), batch_size=batch_size):\n",
    "        # Run optimization op (backprop)\n",
    "        batch_x, batch_y = X_train[start:end], y_train[start:end]\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "#         steps += 1\n",
    "        \n",
    "#     if steps % 20 == 0:\n",
    "\n",
    "    # Calculate batch loss and accuracy\n",
    "    train_loss, train_acc, train_lr, summary_train = sess.run([loss, accuracy, learning_rate, summary], \n",
    "                                           feed_dict={x: X_train, y: y_train, keep_prob: 1.0})\n",
    "    writer_train.add_summary(summary_train, epoch + 1)\n",
    "\n",
    "    print('Epoch %d' % (epoch + 1))\n",
    "    print('learning_rate:', train_lr)\n",
    "    print (\"Training loss:\", train_loss, ', Training acc: ', train_acc)\n",
    "\n",
    "    val_loss, val_acc, summary_test = sess.run([loss, accuracy, summary], feed_dict={x: X_test, \n",
    "                                                                y: y_test,\n",
    "                                                               keep_prob: 1.0})\n",
    "    writer_test.add_summary(summary_test, epoch + 1)\n",
    "    print (\"Test loss:\", val_loss, ', Test acc: ', val_acc)\n",
    "    print('-' * 80)\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for all test samples\n",
    "print (\"Training Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_train,\n",
    "                                  y: y_train,\n",
    "                                 keep_prob: 1.0}))\n",
    "print (\"Test Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_test,\n",
    "                                  y: y_test,\n",
    "                                 keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['serif'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEECAYAAAAPo8LjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c0e0438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess.close()\n",
    "\n",
    "arr5 = np.arange(0, 3600)\n",
    "arr5 = 0.0075 * 0.99 ** (arr5/75)\n",
    "plt.plot(arr5, '--'),\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
