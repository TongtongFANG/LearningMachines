{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('nn_simple_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('nn_simple_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_all = features\n",
    "y_all = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10052, 1280)\n",
      "(10052, 5)\n",
      "(1774, 1280)\n",
      "(1774, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, train_size=.85, random_state=round(time.time()))\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_not_test, y_not_rest, stratify=y_not_rest, train_size=.9, random_state=round(time.time()))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# starter_learning_rate = 0.00\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            50, 0.99, staircase=True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 40 * 32\n",
    "n_classes = 5\n",
    "dropout = .8 # Dropout, probability to keep units\n",
    "\n",
    "# 1. Define Variables and Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "def build_model(x, dropout, activation):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 40, 32, 1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, 4, 5, activation=activation)\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, 8, 3, activation=activation)\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "    conv2 = tf.nn.dropout(conv2, dropout)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "    fc1 = tf.layers.dense(fc1, 128, activation=activation)\n",
    "    fc1 = tf.layers.batch_normalization(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    fc2 = tf.layers.dense(fc1, 64, activation=activation)\n",
    "    fc2 = tf.layers.batch_normalization(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    out = tf.layers.dense(fc2, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "predictions = build_model(x, keep_prob, activation=tf.nn.relu)\n",
    "# 3. Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y), name='loss')\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "\n",
    "# 4. Define the accuracy \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "# 5. Define an optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "def feed_next_batch(train_size, batch_size=64):\n",
    "    \n",
    "    start = 0\n",
    "    while start < train_size:\n",
    "        yield start, start + batch_size\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 1.60638 , Training acc:  0.235177\n",
      "Test loss: 1.60599 , Test acc:  0.239008\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2\n",
      "Training loss: 1.53609 , Training acc:  0.341325\n",
      "Test loss: 1.53647 , Test acc:  0.332018\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3\n",
      "Training loss: 1.31836 , Training acc:  0.490947\n",
      "Test loss: 1.32612 , Test acc:  0.487599\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4\n",
      "Training loss: 1.13889 , Training acc:  0.579686\n",
      "Test loss: 1.16944 , Test acc:  0.564262\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5\n",
      "Training loss: 0.94327 , Training acc:  0.683148\n",
      "Test loss: 0.974173 , Test acc:  0.667418\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6\n",
      "Training loss: 0.883744 , Training acc:  0.682451\n",
      "Test loss: 0.90984 , Test acc:  0.6708\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7\n",
      "Training loss: 0.663398 , Training acc:  0.774572\n",
      "Test loss: 0.701597 , Test acc:  0.759865\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8\n",
      "Training loss: 0.521168 , Training acc:  0.836649\n",
      "Test loss: 0.554394 , Test acc:  0.815107\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9\n",
      "Training loss: 0.514428 , Training acc:  0.829089\n",
      "Test loss: 0.549692 , Test acc:  0.807215\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10\n",
      "Training loss: 0.389006 , Training acc:  0.873558\n",
      "Test loss: 0.438728 , Test acc:  0.856821\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11\n",
      "Training loss: 0.36522 , Training acc:  0.875547\n",
      "Test loss: 0.422587 , Test acc:  0.853439\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12\n",
      "Training loss: 0.330773 , Training acc:  0.896836\n",
      "Test loss: 0.391673 , Test acc:  0.869222\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13\n",
      "Training loss: 0.347561 , Training acc:  0.879924\n",
      "Test loss: 0.404764 , Test acc:  0.852875\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14\n",
      "Training loss: 0.29716 , Training acc:  0.908874\n",
      "Test loss: 0.374201 , Test acc:  0.870349\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15\n",
      "Training loss: 0.255274 , Training acc:  0.920215\n",
      "Test loss: 0.328307 , Test acc:  0.888952\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16\n",
      "Training loss: 0.241588 , Training acc:  0.924592\n",
      "Test loss: 0.313185 , Test acc:  0.899098\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17\n",
      "Training loss: 0.227962 , Training acc:  0.931456\n",
      "Test loss: 0.320281 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18\n",
      "Training loss: 0.213646 , Training acc:  0.93086\n",
      "Test loss: 0.295283 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19\n",
      "Training loss: 0.195527 , Training acc:  0.939614\n",
      "Test loss: 0.295679 , Test acc:  0.894589\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20\n",
      "Training loss: 0.173317 , Training acc:  0.943394\n",
      "Test loss: 0.271196 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 21\n",
      "Training loss: 0.188066 , Training acc:  0.9424\n",
      "Test loss: 0.287223 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 22\n",
      "Training loss: 0.158461 , Training acc:  0.955332\n",
      "Test loss: 0.256435 , Test acc:  0.911499\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 23\n",
      "Training loss: 0.143726 , Training acc:  0.958516\n",
      "Test loss: 0.250623 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 24\n",
      "Training loss: 0.132348 , Training acc:  0.962396\n",
      "Test loss: 0.243234 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25\n",
      "Training loss: 0.130002 , Training acc:  0.961699\n",
      "Test loss: 0.246965 , Test acc:  0.911499\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 26\n",
      "Training loss: 0.120111 , Training acc:  0.966275\n",
      "Test loss: 0.239872 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 27\n",
      "Training loss: 0.131163 , Training acc:  0.961401\n",
      "Test loss: 0.259504 , Test acc:  0.907554\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 28\n",
      "Training loss: 0.105048 , Training acc:  0.967768\n",
      "Test loss: 0.23958 , Test acc:  0.910936\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 29\n",
      "Training loss: 0.107707 , Training acc:  0.970354\n",
      "Test loss: 0.243184 , Test acc:  0.915445\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30\n",
      "Training loss: 0.107933 , Training acc:  0.974532\n",
      "Test loss: 0.242821 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 31\n",
      "Training loss: 0.0958896 , Training acc:  0.975129\n",
      "Test loss: 0.238097 , Test acc:  0.910936\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 32\n",
      "Training loss: 0.0962502 , Training acc:  0.97115\n",
      "Test loss: 0.243491 , Test acc:  0.906426\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 33\n",
      "Training loss: 0.0857712 , Training acc:  0.979308\n",
      "Test loss: 0.232628 , Test acc:  0.922773\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 34\n",
      "Training loss: 0.0791615 , Training acc:  0.978512\n",
      "Test loss: 0.224573 , Test acc:  0.925592\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35\n",
      "Training loss: 0.0748002 , Training acc:  0.981695\n",
      "Test loss: 0.228877 , Test acc:  0.923337\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 36\n",
      "Training loss: 0.082689 , Training acc:  0.978114\n",
      "Test loss: 0.236737 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 37\n",
      "Training loss: 0.0726063 , Training acc:  0.982093\n",
      "Test loss: 0.236709 , Test acc:  0.921082\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 38\n",
      "Training loss: 0.063167 , Training acc:  0.984182\n",
      "Test loss: 0.228237 , Test acc:  0.92221\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 39\n",
      "Training loss: 0.0685693 , Training acc:  0.982292\n",
      "Test loss: 0.239525 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 40\n",
      "Training loss: 0.0628016 , Training acc:  0.986669\n",
      "Test loss: 0.22869 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "Optimization Finished!\n",
      "Training Accuracy: 0.986669\n",
      "Test Accuracy: 0.916573\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer_train = tf.summary.FileWriter('./log/' + 'run_1/train', graph=sess.graph)\n",
    "writer_test = tf.summary.FileWriter('./log/' + 'run_1/test', graph=sess.graph)\n",
    "\n",
    "# Keep training until reach max iterations\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train =  X_train[indices], y_train[indices]\n",
    "\n",
    "    for start, end in feed_next_batch(len(X_train), batch_size=batch_size):\n",
    "        # Run optimization op (backprop)\n",
    "        batch_x, batch_y = X_train[start:end], y_train[start:end]\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "    # Calculate batch loss and accuracy\n",
    "    train_loss, train_acc, summary_train = sess.run([loss, accuracy, summary], \n",
    "                                           feed_dict={x: X_train, y: y_train, keep_prob: 1.0})\n",
    "    writer_train.add_summary(summary_train, epoch)\n",
    "    \n",
    "    print('Epoch %d' % (epoch + 1))\n",
    "    if type(learning_rate) is not float:\n",
    "        print('learning_rate:', sess.run(learning_rate))\n",
    "\n",
    "    print (\"Training loss:\", train_loss, ', Training acc: ', train_acc)\n",
    "\n",
    "    val_loss, val_acc, summary_test = sess.run([loss, accuracy, summary], feed_dict={x: X_test, \n",
    "                                                                y: y_test,\n",
    "                                                               keep_prob: 1.0})\n",
    "    \n",
    "    writer_test.add_summary(summary_test, epoch)\n",
    "    print (\"Test loss:\", val_loss, ', Test acc: ', val_acc)\n",
    "    print('-' * 80)\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for all test samples\n",
    "print (\"Training Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_train,\n",
    "                                  y: y_train,\n",
    "                                 keep_prob: 1.0}))\n",
    "print (\"Test Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_test,\n",
    "                                  y: y_test,\n",
    "                                 keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
