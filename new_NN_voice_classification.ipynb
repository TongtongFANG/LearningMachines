{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('nn_simple_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('nn_simple_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_all = features\n",
    "y_all = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10052, 1280)\n",
      "(10052, 5)\n",
      "(1774, 1280)\n",
      "(1774, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, train_size=.85, random_state=round(time.time()))\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_not_test, y_not_rest, stratify=y_not_rest, train_size=.9, random_state=round(time.time()))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# learning_rate = 0.005\n",
    "\n",
    "with tf.name_scope(\"learning_rate\"):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.0075\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               75, 0.99, staircase=True)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 40 * 32\n",
    "n_classes = 5\n",
    "dropout = .8 # Dropout, probability to keep units\n",
    "\n",
    "# 1. Define Variables and Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "def build_model(x, dropout, activation):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 40, 32, 1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, 4, 5, activation=activation)\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, 8, 3, activation=activation)\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "    conv2 = tf.nn.dropout(conv2, dropout)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "    fc1 = tf.layers.dense(fc1, 128, activation=activation)\n",
    "    fc1 = tf.layers.batch_normalization(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    fc2 = tf.layers.dense(fc1, 64, activation=activation)\n",
    "    fc2 = tf.layers.batch_normalization(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    out = tf.layers.dense(fc2, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "predictions = build_model(x, keep_prob, activation=tf.nn.relu)\n",
    "# 3. Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y), name='loss')\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "\n",
    "# 4. Define the accuracy \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "# 5. Define an optimizer\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "def feed_next_batch(train_size, batch_size=64):\n",
    "    \n",
    "    start = 0\n",
    "    while start < train_size:\n",
    "        yield start, start + batch_size\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 20\n",
      "learning_rate: 0.0075\n",
      "Training loss: 1.64088 , Training acc:  0.193593\n",
      "Test loss: 1.63957 , Test acc:  0.192785\n",
      "--------------------------------------------------------------------------------\n",
      "steps 40\n",
      "learning_rate: 0.0075\n",
      "Training loss: 1.60914 , Training acc:  0.214385\n",
      "Test loss: 1.60992 , Test acc:  0.225479\n",
      "--------------------------------------------------------------------------------\n",
      "steps 60\n",
      "learning_rate: 0.0075\n",
      "Training loss: 1.61856 , Training acc:  0.203044\n",
      "Test loss: 1.6177 , Test acc:  0.201804\n",
      "--------------------------------------------------------------------------------\n",
      "steps 80\n",
      "learning_rate: 0.007425\n",
      "Training loss: 1.60343 , Training acc:  0.228611\n",
      "Test loss: 1.60322 , Test acc:  0.219278\n",
      "--------------------------------------------------------------------------------\n",
      "steps 100\n",
      "learning_rate: 0.007425\n",
      "Training loss: 1.59098 , Training acc:  0.25199\n",
      "Test loss: 1.59245 , Test acc:  0.242954\n",
      "--------------------------------------------------------------------------------\n",
      "steps 120\n",
      "learning_rate: 0.007425\n",
      "Training loss: 1.55333 , Training acc:  0.259252\n",
      "Test loss: 1.55608 , Test acc:  0.25761\n",
      "--------------------------------------------------------------------------------\n",
      "steps 140\n",
      "learning_rate: 0.007425\n",
      "Training loss: 1.54953 , Training acc:  0.273478\n",
      "Test loss: 1.55901 , Test acc:  0.25761\n",
      "--------------------------------------------------------------------------------\n",
      "steps 160\n",
      "learning_rate: 0.00735075\n",
      "Training loss: 1.48783 , Training acc:  0.322224\n",
      "Test loss: 1.49951 , Test acc:  0.317926\n",
      "--------------------------------------------------------------------------------\n",
      "steps 180\n",
      "learning_rate: 0.00735075\n",
      "Training loss: 1.44475 , Training acc:  0.307203\n",
      "Test loss: 1.472 , Test acc:  0.299324\n",
      "--------------------------------------------------------------------------------\n",
      "steps 200\n",
      "learning_rate: 0.00735075\n",
      "Training loss: 1.38879 , Training acc:  0.369877\n",
      "Test loss: 1.40202 , Test acc:  0.377678\n",
      "--------------------------------------------------------------------------------\n",
      "steps 220\n",
      "learning_rate: 0.00735075\n",
      "Training loss: 1.31196 , Training acc:  0.405193\n",
      "Test loss: 1.32771 , Test acc:  0.401917\n",
      "--------------------------------------------------------------------------------\n",
      "steps 240\n",
      "learning_rate: 0.00727724\n",
      "Training loss: 1.35289 , Training acc:  0.362018\n",
      "Test loss: 1.3702 , Test acc:  0.367531\n",
      "--------------------------------------------------------------------------------\n",
      "steps 260\n",
      "learning_rate: 0.00727724\n",
      "Training loss: 1.28749 , Training acc:  0.371468\n",
      "Test loss: 1.29622 , Test acc:  0.369786\n",
      "--------------------------------------------------------------------------------\n",
      "steps 280\n",
      "learning_rate: 0.00727724\n",
      "Training loss: 1.2838 , Training acc:  0.384501\n",
      "Test loss: 1.30702 , Test acc:  0.38106\n",
      "--------------------------------------------------------------------------------\n",
      "steps 300\n",
      "learning_rate: 0.00720447\n",
      "Training loss: 1.25439 , Training acc:  0.43454\n",
      "Test loss: 1.27317 , Test acc:  0.42221\n",
      "--------------------------------------------------------------------------------\n",
      "steps 320\n",
      "learning_rate: 0.00720447\n",
      "Training loss: 1.14643 , Training acc:  0.504079\n",
      "Test loss: 1.15788 , Test acc:  0.503946\n",
      "--------------------------------------------------------------------------------\n",
      "steps 340\n",
      "learning_rate: 0.00720447\n",
      "Training loss: 1.12222 , Training acc:  0.490748\n",
      "Test loss: 1.14119 , Test acc:  0.483653\n",
      "--------------------------------------------------------------------------------\n",
      "steps 360\n",
      "learning_rate: 0.00720447\n",
      "Training loss: 1.0532 , Training acc:  0.548747\n",
      "Test loss: 1.06769 , Test acc:  0.551297\n",
      "--------------------------------------------------------------------------------\n",
      "steps 380\n",
      "learning_rate: 0.00713243\n",
      "Training loss: 1.04525 , Training acc:  0.549642\n",
      "Test loss: 1.06351 , Test acc:  0.532131\n",
      "--------------------------------------------------------------------------------\n",
      "steps 400\n",
      "learning_rate: 0.00713243\n",
      "Training loss: 1.20526 , Training acc:  0.443693\n",
      "Test loss: 1.21887 , Test acc:  0.43743\n",
      "--------------------------------------------------------------------------------\n",
      "steps 420\n",
      "learning_rate: 0.00713243\n",
      "Training loss: 1.03942 , Training acc:  0.519797\n",
      "Test loss: 1.05329 , Test acc:  0.51522\n",
      "--------------------------------------------------------------------------------\n",
      "steps 440\n",
      "learning_rate: 0.00713243\n",
      "Training loss: 0.973623 , Training acc:  0.617887\n",
      "Test loss: 1.0091 , Test acc:  0.605975\n",
      "--------------------------------------------------------------------------------\n",
      "steps 460\n",
      "learning_rate: 0.0070611\n",
      "Training loss: 1.19719 , Training acc:  0.460306\n",
      "Test loss: 1.24751 , Test acc:  0.454904\n",
      "--------------------------------------------------------------------------------\n",
      "steps 480\n",
      "learning_rate: 0.0070611\n",
      "Training loss: 0.849496 , Training acc:  0.692002\n",
      "Test loss: 0.894284 , Test acc:  0.666291\n",
      "--------------------------------------------------------------------------------\n",
      "steps 500\n",
      "learning_rate: 0.0070611\n",
      "Training loss: 0.803351 , Training acc:  0.723538\n",
      "Test loss: 0.847788 , Test acc:  0.691657\n",
      "--------------------------------------------------------------------------------\n",
      "steps 520\n",
      "learning_rate: 0.0070611\n",
      "Training loss: 0.775867 , Training acc:  0.709511\n",
      "Test loss: 0.8217 , Test acc:  0.679256\n",
      "--------------------------------------------------------------------------------\n",
      "steps 540\n",
      "learning_rate: 0.00699049\n",
      "Training loss: 0.706288 , Training acc:  0.744429\n",
      "Test loss: 0.762959 , Test acc:  0.709696\n",
      "--------------------------------------------------------------------------------\n",
      "steps 560\n",
      "learning_rate: 0.00699049\n",
      "Training loss: 0.721779 , Training acc:  0.743832\n",
      "Test loss: 0.765053 , Test acc:  0.722661\n",
      "--------------------------------------------------------------------------------\n",
      "steps 580\n",
      "learning_rate: 0.00699049\n",
      "Training loss: 0.66218 , Training acc:  0.768703\n",
      "Test loss: 0.7038 , Test acc:  0.7469\n",
      "--------------------------------------------------------------------------------\n",
      "steps 600\n",
      "learning_rate: 0.00692059\n",
      "Training loss: 0.6728 , Training acc:  0.760346\n",
      "Test loss: 0.723767 , Test acc:  0.736753\n",
      "--------------------------------------------------------------------------------\n",
      "steps 620\n",
      "learning_rate: 0.00692059\n",
      "Training loss: 0.642635 , Training acc:  0.787107\n",
      "Test loss: 0.690491 , Test acc:  0.767193\n",
      "--------------------------------------------------------------------------------\n",
      "steps 640\n",
      "learning_rate: 0.00692059\n",
      "Training loss: 0.602377 , Training acc:  0.789097\n",
      "Test loss: 0.661266 , Test acc:  0.761556\n",
      "--------------------------------------------------------------------------------\n",
      "steps 660\n",
      "learning_rate: 0.00692059\n",
      "Training loss: 0.565663 , Training acc:  0.797851\n",
      "Test loss: 0.626126 , Test acc:  0.755355\n",
      "--------------------------------------------------------------------------------\n",
      "steps 680\n",
      "learning_rate: 0.00685138\n",
      "Training loss: 0.573445 , Training acc:  0.798846\n",
      "Test loss: 0.629524 , Test acc:  0.785795\n",
      "--------------------------------------------------------------------------------\n",
      "steps 700\n",
      "learning_rate: 0.00685138\n",
      "Training loss: 0.566877 , Training acc:  0.790191\n",
      "Test loss: 0.634813 , Test acc:  0.766065\n",
      "--------------------------------------------------------------------------------\n",
      "steps 720\n",
      "learning_rate: 0.00685138\n",
      "Training loss: 0.505044 , Training acc:  0.837545\n",
      "Test loss: 0.566378 , Test acc:  0.818489\n",
      "--------------------------------------------------------------------------------\n",
      "steps 740\n",
      "learning_rate: 0.00685138\n",
      "Training loss: 0.511607 , Training acc:  0.835754\n",
      "Test loss: 0.575208 , Test acc:  0.801015\n",
      "--------------------------------------------------------------------------------\n",
      "steps 760\n",
      "learning_rate: 0.00678287\n",
      "Training loss: 0.590729 , Training acc:  0.792877\n",
      "Test loss: 0.651933 , Test acc:  0.761556\n",
      "--------------------------------------------------------------------------------\n",
      "steps 780\n",
      "learning_rate: 0.00678287\n",
      "Training loss: 0.517736 , Training acc:  0.815559\n",
      "Test loss: 0.588306 , Test acc:  0.787486\n",
      "--------------------------------------------------------------------------------\n",
      "steps 800\n",
      "learning_rate: 0.00678287\n",
      "Training loss: 0.506119 , Training acc:  0.836053\n",
      "Test loss: 0.575268 , Test acc:  0.806088\n",
      "--------------------------------------------------------------------------------\n",
      "steps 820\n",
      "learning_rate: 0.00678287\n",
      "Training loss: 0.51519 , Training acc:  0.810585\n",
      "Test loss: 0.583352 , Test acc:  0.789741\n",
      "--------------------------------------------------------------------------------\n",
      "steps 840\n",
      "learning_rate: 0.00671504\n",
      "Training loss: 0.483181 , Training acc:  0.831576\n",
      "Test loss: 0.553569 , Test acc:  0.803269\n",
      "--------------------------------------------------------------------------------\n",
      "steps 860\n",
      "learning_rate: 0.00671504\n",
      "Training loss: 0.423215 , Training acc:  0.855352\n",
      "Test loss: 0.489907 , Test acc:  0.836528\n",
      "--------------------------------------------------------------------------------\n",
      "steps 880\n",
      "learning_rate: 0.00671504\n",
      "Training loss: 0.412946 , Training acc:  0.864306\n",
      "Test loss: 0.470439 , Test acc:  0.838219\n",
      "--------------------------------------------------------------------------------\n",
      "steps 900\n",
      "learning_rate: 0.00664789\n",
      "Training loss: 0.428797 , Training acc:  0.85197\n",
      "Test loss: 0.4953 , Test acc:  0.827508\n",
      "--------------------------------------------------------------------------------\n",
      "steps 920\n",
      "learning_rate: 0.00664789\n",
      "Training loss: 0.397354 , Training acc:  0.868882\n",
      "Test loss: 0.468666 , Test acc:  0.841037\n",
      "--------------------------------------------------------------------------------\n",
      "steps 940\n",
      "learning_rate: 0.00664789\n",
      "Training loss: 0.397907 , Training acc:  0.872065\n",
      "Test loss: 0.46804 , Test acc:  0.832582\n",
      "--------------------------------------------------------------------------------\n",
      "steps 960\n",
      "learning_rate: 0.00664789\n",
      "Training loss: 0.421995 , Training acc:  0.855253\n",
      "Test loss: 0.489804 , Test acc:  0.834837\n",
      "--------------------------------------------------------------------------------\n",
      "steps 980\n",
      "learning_rate: 0.00658141\n",
      "Training loss: 0.407489 , Training acc:  0.870275\n",
      "Test loss: 0.470448 , Test acc:  0.841037\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1000\n",
      "learning_rate: 0.00658141\n",
      "Training loss: 0.402777 , Training acc:  0.872165\n",
      "Test loss: 0.47013 , Test acc:  0.846674\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1020\n",
      "learning_rate: 0.00658141\n",
      "Training loss: 0.371361 , Training acc:  0.878631\n",
      "Test loss: 0.444524 , Test acc:  0.842165\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1040\n",
      "learning_rate: 0.00658141\n",
      "Training loss: 0.393156 , Training acc:  0.86351\n",
      "Test loss: 0.470298 , Test acc:  0.826381\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1060\n",
      "learning_rate: 0.00651559\n",
      "Training loss: 0.355151 , Training acc:  0.87893\n",
      "Test loss: 0.446206 , Test acc:  0.850056\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1080\n",
      "learning_rate: 0.00651559\n",
      "Training loss: 0.372469 , Training acc:  0.879029\n",
      "Test loss: 0.44794 , Test acc:  0.851747\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1100\n",
      "learning_rate: 0.00651559\n",
      "Training loss: 0.365195 , Training acc:  0.886988\n",
      "Test loss: 0.446728 , Test acc:  0.843856\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1120\n",
      "learning_rate: 0.00651559\n",
      "Training loss: 0.351209 , Training acc:  0.883207\n",
      "Test loss: 0.438496 , Test acc:  0.845547\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1140\n",
      "learning_rate: 0.00645044\n",
      "Training loss: 0.340269 , Training acc:  0.887684\n",
      "Test loss: 0.416232 , Test acc:  0.860767\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1160\n",
      "learning_rate: 0.00645044\n",
      "Training loss: 0.350264 , Training acc:  0.884003\n",
      "Test loss: 0.418423 , Test acc:  0.857384\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1180\n",
      "learning_rate: 0.00645044\n",
      "Training loss: 0.335731 , Training acc:  0.889276\n",
      "Test loss: 0.426887 , Test acc:  0.851747\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1200\n",
      "learning_rate: 0.00638593\n",
      "Training loss: 0.319102 , Training acc:  0.892957\n",
      "Test loss: 0.398835 , Test acc:  0.864149\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1220\n",
      "learning_rate: 0.00638593\n",
      "Training loss: 0.353449 , Training acc:  0.876542\n",
      "Test loss: 0.45052 , Test acc:  0.833145\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1240\n",
      "learning_rate: 0.00638593\n",
      "Training loss: 0.311431 , Training acc:  0.894449\n",
      "Test loss: 0.399763 , Test acc:  0.861894\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1260\n",
      "learning_rate: 0.00638593\n",
      "Training loss: 0.305679 , Training acc:  0.901512\n",
      "Test loss: 0.38919 , Test acc:  0.853439\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1280\n",
      "learning_rate: 0.00632207\n",
      "Training loss: 0.328593 , Training acc:  0.901711\n",
      "Test loss: 0.415172 , Test acc:  0.863585\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1300\n",
      "learning_rate: 0.00632207\n",
      "Training loss: 0.281222 , Training acc:  0.903203\n",
      "Test loss: 0.371667 , Test acc:  0.868095\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1320\n",
      "learning_rate: 0.00632207\n",
      "Training loss: 0.287451 , Training acc:  0.903999\n",
      "Test loss: 0.378452 , Test acc:  0.864149\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1340\n",
      "learning_rate: 0.00632207\n",
      "Training loss: 0.291739 , Training acc:  0.905392\n",
      "Test loss: 0.375639 , Test acc:  0.852311\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1360\n",
      "learning_rate: 0.00625885\n",
      "Training loss: 0.274458 , Training acc:  0.910366\n",
      "Test loss: 0.354147 , Test acc:  0.875423\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1380\n",
      "learning_rate: 0.00625885\n",
      "Training loss: 0.294114 , Training acc:  0.896637\n",
      "Test loss: 0.390689 , Test acc:  0.858512\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1400\n",
      "learning_rate: 0.00625885\n",
      "Training loss: 0.28469 , Training acc:  0.905591\n",
      "Test loss: 0.368886 , Test acc:  0.867531\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1420\n",
      "learning_rate: 0.00625885\n",
      "Training loss: 0.255723 , Training acc:  0.918922\n",
      "Test loss: 0.344275 , Test acc:  0.871477\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1440\n",
      "learning_rate: 0.00619627\n",
      "Training loss: 0.282347 , Training acc:  0.902706\n",
      "Test loss: 0.37299 , Test acc:  0.866404\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1460\n",
      "learning_rate: 0.00619627\n",
      "Training loss: 0.249572 , Training acc:  0.915937\n",
      "Test loss: 0.332648 , Test acc:  0.88106\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1480\n",
      "learning_rate: 0.00619627\n",
      "Training loss: 0.275524 , Training acc:  0.909073\n",
      "Test loss: 0.373234 , Test acc:  0.867531\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1500\n",
      "learning_rate: 0.0061343\n",
      "Training loss: 0.336379 , Training acc:  0.888181\n",
      "Test loss: 0.442949 , Test acc:  0.845547\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1520\n",
      "learning_rate: 0.0061343\n",
      "Training loss: 0.280012 , Training acc:  0.907581\n",
      "Test loss: 0.386246 , Test acc:  0.870349\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1540\n",
      "learning_rate: 0.0061343\n",
      "Training loss: 0.241485 , Training acc:  0.920812\n",
      "Test loss: 0.346943 , Test acc:  0.884442\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1560\n",
      "learning_rate: 0.0061343\n",
      "Training loss: 0.256821 , Training acc:  0.911958\n",
      "Test loss: 0.358134 , Test acc:  0.873168\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1580\n",
      "learning_rate: 0.00607296\n",
      "Training loss: 0.316276 , Training acc:  0.896339\n",
      "Test loss: 0.416727 , Test acc:  0.853439\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1600\n",
      "learning_rate: 0.00607296\n",
      "Training loss: 0.233816 , Training acc:  0.928273\n",
      "Test loss: 0.327466 , Test acc:  0.881623\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1620\n",
      "learning_rate: 0.00607296\n",
      "Training loss: 0.217447 , Training acc:  0.925288\n",
      "Test loss: 0.303815 , Test acc:  0.888952\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1640\n",
      "learning_rate: 0.00607296\n",
      "Training loss: 0.236702 , Training acc:  0.929666\n",
      "Test loss: 0.332416 , Test acc:  0.888388\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1660\n",
      "learning_rate: 0.00601223\n",
      "Training loss: 0.232317 , Training acc:  0.929467\n",
      "Test loss: 0.331567 , Test acc:  0.890079\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1680\n",
      "learning_rate: 0.00601223\n",
      "Training loss: 0.218181 , Training acc:  0.927975\n",
      "Test loss: 0.320169 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1700\n",
      "learning_rate: 0.00601223\n",
      "Training loss: 0.242457 , Training acc:  0.919817\n",
      "Test loss: 0.357411 , Test acc:  0.881623\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1720\n",
      "learning_rate: 0.00601223\n",
      "Training loss: 0.196683 , Training acc:  0.932053\n",
      "Test loss: 0.29639 , Test acc:  0.896843\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1740\n",
      "learning_rate: 0.00595211\n",
      "Training loss: 0.22547 , Training acc:  0.922503\n",
      "Test loss: 0.335315 , Test acc:  0.883315\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1760\n",
      "learning_rate: 0.00595211\n",
      "Training loss: 0.202437 , Training acc:  0.93464\n",
      "Test loss: 0.300007 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1780\n",
      "learning_rate: 0.00595211\n",
      "Training loss: 0.216288 , Training acc:  0.932252\n",
      "Test loss: 0.327551 , Test acc:  0.882187\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1800\n",
      "learning_rate: 0.00589259\n",
      "Training loss: 0.237152 , Training acc:  0.914843\n",
      "Test loss: 0.346631 , Test acc:  0.871477\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1820\n",
      "learning_rate: 0.00589259\n",
      "Training loss: 0.189718 , Training acc:  0.938918\n",
      "Test loss: 0.304004 , Test acc:  0.886133\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1840\n",
      "learning_rate: 0.00589259\n",
      "Training loss: 0.183687 , Training acc:  0.94419\n",
      "Test loss: 0.300607 , Test acc:  0.893461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1860\n",
      "learning_rate: 0.00589259\n",
      "Training loss: 0.188842 , Training acc:  0.942101\n",
      "Test loss: 0.305539 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1880\n",
      "learning_rate: 0.00583366\n",
      "Training loss: 0.189358 , Training acc:  0.943892\n",
      "Test loss: 0.298662 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1900\n",
      "learning_rate: 0.00583366\n",
      "Training loss: 0.191653 , Training acc:  0.93842\n",
      "Test loss: 0.310813 , Test acc:  0.887824\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1920\n",
      "learning_rate: 0.00583366\n",
      "Training loss: 0.18857 , Training acc:  0.942499\n",
      "Test loss: 0.313726 , Test acc:  0.892334\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1940\n",
      "learning_rate: 0.00583366\n",
      "Training loss: 0.185073 , Training acc:  0.942201\n",
      "Test loss: 0.299921 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1960\n",
      "learning_rate: 0.00577532\n",
      "Training loss: 0.187782 , Training acc:  0.938719\n",
      "Test loss: 0.306961 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 1980\n",
      "learning_rate: 0.00577532\n",
      "Training loss: 0.18664 , Training acc:  0.942996\n",
      "Test loss: 0.296498 , Test acc:  0.895152\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2000\n",
      "learning_rate: 0.00577532\n",
      "Training loss: 0.172994 , Training acc:  0.941405\n",
      "Test loss: 0.297191 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2020\n",
      "learning_rate: 0.00577532\n",
      "Training loss: 0.172387 , Training acc:  0.943593\n",
      "Test loss: 0.292259 , Test acc:  0.894025\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2040\n",
      "learning_rate: 0.00571757\n",
      "Training loss: 0.184018 , Training acc:  0.939316\n",
      "Test loss: 0.301565 , Test acc:  0.89177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2060\n",
      "learning_rate: 0.00571757\n",
      "Training loss: 0.176658 , Training acc:  0.946578\n",
      "Test loss: 0.292292 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2080\n",
      "learning_rate: 0.00571757\n",
      "Training loss: 0.167917 , Training acc:  0.948468\n",
      "Test loss: 0.28905 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2100\n",
      "learning_rate: 0.0056604\n",
      "Training loss: 0.16071 , Training acc:  0.947672\n",
      "Test loss: 0.282044 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2120\n",
      "learning_rate: 0.0056604\n",
      "Training loss: 0.175995 , Training acc:  0.949662\n",
      "Test loss: 0.300823 , Test acc:  0.892897\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2140\n",
      "learning_rate: 0.0056604\n",
      "Training loss: 0.170143 , Training acc:  0.942101\n",
      "Test loss: 0.301935 , Test acc:  0.892897\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2160\n",
      "learning_rate: 0.0056604\n",
      "Training loss: 0.156556 , Training acc:  0.949065\n",
      "Test loss: 0.287079 , Test acc:  0.901353\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2180\n",
      "learning_rate: 0.00560379\n",
      "Training loss: 0.173781 , Training acc:  0.94807\n",
      "Test loss: 0.28232 , Test acc:  0.900225\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2200\n",
      "learning_rate: 0.00560379\n",
      "Training loss: 0.166258 , Training acc:  0.951552\n",
      "Test loss: 0.283587 , Test acc:  0.893461\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2220\n",
      "learning_rate: 0.00560379\n",
      "Training loss: 0.167149 , Training acc:  0.945981\n",
      "Test loss: 0.296872 , Test acc:  0.897971\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2240\n",
      "learning_rate: 0.00560379\n",
      "Training loss: 0.193148 , Training acc:  0.933844\n",
      "Test loss: 0.303671 , Test acc:  0.889515\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2260\n",
      "learning_rate: 0.00554775\n",
      "Training loss: 0.172462 , Training acc:  0.947075\n",
      "Test loss: 0.316392 , Test acc:  0.892334\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2280\n",
      "learning_rate: 0.00554775\n",
      "Training loss: 0.14862 , Training acc:  0.954835\n",
      "Test loss: 0.276102 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2300\n",
      "learning_rate: 0.00554775\n",
      "Training loss: 0.165214 , Training acc:  0.941007\n",
      "Test loss: 0.29819 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2320\n",
      "learning_rate: 0.00554775\n",
      "Training loss: 0.170898 , Training acc:  0.946279\n",
      "Test loss: 0.326515 , Test acc:  0.889515\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2340\n",
      "learning_rate: 0.00549228\n",
      "Training loss: 0.152969 , Training acc:  0.954835\n",
      "Test loss: 0.279182 , Test acc:  0.900789\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2360\n",
      "learning_rate: 0.00549228\n",
      "Training loss: 0.156262 , Training acc:  0.948269\n",
      "Test loss: 0.300299 , Test acc:  0.891206\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2380\n",
      "learning_rate: 0.00549228\n",
      "Training loss: 0.188164 , Training acc:  0.935336\n",
      "Test loss: 0.342886 , Test acc:  0.885569\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2400\n",
      "learning_rate: 0.00543735\n",
      "Training loss: 0.160489 , Training acc:  0.954735\n",
      "Test loss: 0.289711 , Test acc:  0.897407\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2420\n",
      "learning_rate: 0.00543735\n",
      "Training loss: 0.144731 , Training acc:  0.956526\n",
      "Test loss: 0.281702 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2440\n",
      "learning_rate: 0.00543735\n",
      "Training loss: 0.156897 , Training acc:  0.947573\n",
      "Test loss: 0.298281 , Test acc:  0.898534\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2460\n",
      "learning_rate: 0.00543735\n",
      "Training loss: 0.131275 , Training acc:  0.959411\n",
      "Test loss: 0.264225 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2480\n",
      "learning_rate: 0.00538298\n",
      "Training loss: 0.156023 , Training acc:  0.94817\n",
      "Test loss: 0.289602 , Test acc:  0.896843\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2500\n",
      "learning_rate: 0.00538298\n",
      "Training loss: 0.145642 , Training acc:  0.955233\n",
      "Test loss: 0.280102 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2520\n",
      "learning_rate: 0.00538298\n",
      "Training loss: 0.128711 , Training acc:  0.960804\n",
      "Test loss: 0.26567 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2540\n",
      "learning_rate: 0.00538298\n",
      "Training loss: 0.126729 , Training acc:  0.960207\n",
      "Test loss: 0.269383 , Test acc:  0.906426\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2560\n",
      "learning_rate: 0.00532915\n",
      "Training loss: 0.123639 , Training acc:  0.965479\n",
      "Test loss: 0.255534 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2580\n",
      "learning_rate: 0.00532915\n",
      "Training loss: 0.120857 , Training acc:  0.963788\n",
      "Test loss: 0.256024 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2600\n",
      "learning_rate: 0.00532915\n",
      "Training loss: 0.122208 , Training acc:  0.967569\n",
      "Test loss: 0.250508 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2620\n",
      "learning_rate: 0.00532915\n",
      "Training loss: 0.136739 , Training acc:  0.961003\n",
      "Test loss: 0.27723 , Test acc:  0.908117\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2640\n",
      "learning_rate: 0.00527586\n",
      "Training loss: 0.114381 , Training acc:  0.96538\n",
      "Test loss: 0.254181 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2660\n",
      "learning_rate: 0.00527586\n",
      "Training loss: 0.13252 , Training acc:  0.960008\n",
      "Test loss: 0.282347 , Test acc:  0.906426\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2680\n",
      "learning_rate: 0.00527586\n",
      "Training loss: 0.115729 , Training acc:  0.966673\n",
      "Test loss: 0.258123 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2700\n",
      "learning_rate: 0.0052231\n",
      "Training loss: 0.118575 , Training acc:  0.967469\n",
      "Test loss: 0.272443 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2720\n",
      "learning_rate: 0.0052231\n",
      "Training loss: 0.126779 , Training acc:  0.95961\n",
      "Test loss: 0.292424 , Test acc:  0.901353\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2740\n",
      "learning_rate: 0.0052231\n",
      "Training loss: 0.129228 , Training acc:  0.961799\n",
      "Test loss: 0.282717 , Test acc:  0.903044\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2760\n",
      "learning_rate: 0.0052231\n",
      "Training loss: 0.129345 , Training acc:  0.965579\n",
      "Test loss: 0.292521 , Test acc:  0.899662\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2780\n",
      "learning_rate: 0.00517087\n",
      "Training loss: 0.111033 , Training acc:  0.966972\n",
      "Test loss: 0.269425 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2800\n",
      "learning_rate: 0.00517087\n",
      "Training loss: 0.112173 , Training acc:  0.966375\n",
      "Test loss: 0.272479 , Test acc:  0.904735\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2820\n",
      "learning_rate: 0.00517087\n",
      "Training loss: 0.116304 , Training acc:  0.968166\n",
      "Test loss: 0.255197 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2840\n",
      "learning_rate: 0.00517087\n",
      "Training loss: 0.111125 , Training acc:  0.970553\n",
      "Test loss: 0.263312 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2860\n",
      "learning_rate: 0.00511916\n",
      "Training loss: 0.112382 , Training acc:  0.965678\n",
      "Test loss: 0.25078 , Test acc:  0.919391\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2880\n",
      "learning_rate: 0.00511916\n",
      "Training loss: 0.119367 , Training acc:  0.965778\n",
      "Test loss: 0.259078 , Test acc:  0.910372\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2900\n",
      "learning_rate: 0.00511916\n",
      "Training loss: 0.117372 , Training acc:  0.965778\n",
      "Test loss: 0.266496 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2920\n",
      "learning_rate: 0.00511916\n",
      "Training loss: 0.100473 , Training acc:  0.969757\n",
      "Test loss: 0.252194 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2940\n",
      "learning_rate: 0.00506797\n",
      "Training loss: 0.111264 , Training acc:  0.963191\n",
      "Test loss: 0.279361 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2960\n",
      "learning_rate: 0.00506797\n",
      "Training loss: 0.101794 , Training acc:  0.969558\n",
      "Test loss: 0.246272 , Test acc:  0.910372\n",
      "--------------------------------------------------------------------------------\n",
      "steps 2980\n",
      "learning_rate: 0.00506797\n",
      "Training loss: 0.102334 , Training acc:  0.97125\n",
      "Test loss: 0.242929 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3000\n",
      "learning_rate: 0.00501729\n",
      "Training loss: 0.111576 , Training acc:  0.968364\n",
      "Test loss: 0.263916 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3020\n",
      "learning_rate: 0.00501729\n",
      "Training loss: 0.104881 , Training acc:  0.968265\n",
      "Test loss: 0.253961 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3040\n",
      "learning_rate: 0.00501729\n",
      "Training loss: 0.11299 , Training acc:  0.967071\n",
      "Test loss: 0.259193 , Test acc:  0.909245\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3060\n",
      "learning_rate: 0.00501729\n",
      "Training loss: 0.110934 , Training acc:  0.964783\n",
      "Test loss: 0.273322 , Test acc:  0.90699\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3080\n",
      "learning_rate: 0.00496712\n",
      "Training loss: 0.0993053 , Training acc:  0.974433\n",
      "Test loss: 0.260172 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3100\n",
      "learning_rate: 0.00496712\n",
      "Training loss: 0.0966408 , Training acc:  0.972045\n",
      "Test loss: 0.251592 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3120\n",
      "learning_rate: 0.00496712\n",
      "Training loss: 0.103096 , Training acc:  0.972543\n",
      "Test loss: 0.250432 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3140\n",
      "learning_rate: 0.00496712\n",
      "Training loss: 0.0941813 , Training acc:  0.973339\n",
      "Test loss: 0.240499 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3160\n",
      "learning_rate: 0.00491745\n",
      "Training loss: 0.0940758 , Training acc:  0.971349\n",
      "Test loss: 0.257302 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3180\n",
      "learning_rate: 0.00491745\n",
      "Training loss: 0.111065 , Training acc:  0.966375\n",
      "Test loss: 0.270619 , Test acc:  0.903608\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3200\n",
      "learning_rate: 0.00491745\n",
      "Training loss: 0.0905548 , Training acc:  0.970852\n",
      "Test loss: 0.255496 , Test acc:  0.915445\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3220\n",
      "learning_rate: 0.00491745\n",
      "Training loss: 0.0986007 , Training acc:  0.969857\n",
      "Test loss: 0.248182 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3240\n",
      "learning_rate: 0.00486827\n",
      "Training loss: 0.103142 , Training acc:  0.969658\n",
      "Test loss: 0.267381 , Test acc:  0.911499\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3260\n",
      "learning_rate: 0.00486827\n",
      "Training loss: 0.0956736 , Training acc:  0.97304\n",
      "Test loss: 0.256505 , Test acc:  0.906426\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3280\n",
      "learning_rate: 0.00486827\n",
      "Training loss: 0.0927451 , Training acc:  0.974234\n",
      "Test loss: 0.256553 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3300\n",
      "learning_rate: 0.00481959\n",
      "Training loss: 0.100135 , Training acc:  0.974433\n",
      "Test loss: 0.255153 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3320\n",
      "learning_rate: 0.00481959\n",
      "Training loss: 0.0924907 , Training acc:  0.973438\n",
      "Test loss: 0.261328 , Test acc:  0.910372\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3340\n",
      "learning_rate: 0.00481959\n",
      "Training loss: 0.0922789 , Training acc:  0.973339\n",
      "Test loss: 0.249296 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3360\n",
      "learning_rate: 0.00481959\n",
      "Training loss: 0.105084 , Training acc:  0.968265\n",
      "Test loss: 0.271989 , Test acc:  0.912063\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3380\n",
      "learning_rate: 0.00477139\n",
      "Training loss: 0.0906767 , Training acc:  0.974333\n",
      "Test loss: 0.241618 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3400\n",
      "learning_rate: 0.00477139\n",
      "Training loss: 0.0838009 , Training acc:  0.978412\n",
      "Test loss: 0.243734 , Test acc:  0.918264\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3420\n",
      "learning_rate: 0.00477139\n",
      "Training loss: 0.0795019 , Training acc:  0.978114\n",
      "Test loss: 0.238295 , Test acc:  0.919955\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3440\n",
      "learning_rate: 0.00477139\n",
      "Training loss: 0.087574 , Training acc:  0.976124\n",
      "Test loss: 0.23469 , Test acc:  0.920519\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3460\n",
      "learning_rate: 0.00472368\n",
      "Training loss: 0.0996179 , Training acc:  0.971349\n",
      "Test loss: 0.254303 , Test acc:  0.905862\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3480\n",
      "learning_rate: 0.00472368\n",
      "Training loss: 0.0814726 , Training acc:  0.975826\n",
      "Test loss: 0.245528 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3500\n",
      "learning_rate: 0.00472368\n",
      "Training loss: 0.100714 , Training acc:  0.970454\n",
      "Test loss: 0.264795 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3520\n",
      "learning_rate: 0.00472368\n",
      "Training loss: 0.0925822 , Training acc:  0.972244\n",
      "Test loss: 0.250039 , Test acc:  0.914318\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3540\n",
      "learning_rate: 0.00467644\n",
      "Training loss: 0.0895412 , Training acc:  0.977815\n",
      "Test loss: 0.247829 , Test acc:  0.912627\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3560\n",
      "learning_rate: 0.00467644\n",
      "Training loss: 0.087482 , Training acc:  0.972941\n",
      "Test loss: 0.251244 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3580\n",
      "learning_rate: 0.00467644\n",
      "Training loss: 0.0824901 , Training acc:  0.974632\n",
      "Test loss: 0.243244 , Test acc:  0.9177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3600\n",
      "learning_rate: 0.00462968\n",
      "Training loss: 0.0896964 , Training acc:  0.972742\n",
      "Test loss: 0.243756 , Test acc:  0.921646\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3620\n",
      "learning_rate: 0.00462968\n",
      "Training loss: 0.089967 , Training acc:  0.97314\n",
      "Test loss: 0.252379 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3640\n",
      "learning_rate: 0.00462968\n",
      "Training loss: 0.0843148 , Training acc:  0.975129\n",
      "Test loss: 0.253501 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3660\n",
      "learning_rate: 0.00462968\n",
      "Training loss: 0.0808227 , Training acc:  0.975428\n",
      "Test loss: 0.250682 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3680\n",
      "learning_rate: 0.00458338\n",
      "Training loss: 0.0723914 , Training acc:  0.9807\n",
      "Test loss: 0.23507 , Test acc:  0.918264\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3700\n",
      "learning_rate: 0.00458338\n",
      "Training loss: 0.0766183 , Training acc:  0.978213\n",
      "Test loss: 0.22841 , Test acc:  0.925592\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3720\n",
      "learning_rate: 0.00458338\n",
      "Training loss: 0.0791028 , Training acc:  0.975627\n",
      "Test loss: 0.247653 , Test acc:  0.913191\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3740\n",
      "learning_rate: 0.00458338\n",
      "Training loss: 0.0682558 , Training acc:  0.980103\n",
      "Test loss: 0.233608 , Test acc:  0.920519\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3760\n",
      "learning_rate: 0.00453755\n",
      "Training loss: 0.0716122 , Training acc:  0.980103\n",
      "Test loss: 0.232476 , Test acc:  0.921082\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3780\n",
      "learning_rate: 0.00453755\n",
      "Training loss: 0.0849335 , Training acc:  0.974532\n",
      "Test loss: 0.254559 , Test acc:  0.908681\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3800\n",
      "learning_rate: 0.00453755\n",
      "Training loss: 0.0805807 , Training acc:  0.97493\n",
      "Test loss: 0.238212 , Test acc:  0.917136\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3820\n",
      "learning_rate: 0.00453755\n",
      "Training loss: 0.0908051 , Training acc:  0.974333\n",
      "Test loss: 0.265783 , Test acc:  0.904171\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3840\n",
      "learning_rate: 0.00449217\n",
      "Training loss: 0.0731802 , Training acc:  0.980402\n",
      "Test loss: 0.243491 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3860\n",
      "learning_rate: 0.00449217\n",
      "Training loss: 0.0732217 , Training acc:  0.981496\n",
      "Test loss: 0.249653 , Test acc:  0.916009\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3880\n",
      "learning_rate: 0.00449217\n",
      "Training loss: 0.0621343 , Training acc:  0.983386\n",
      "Test loss: 0.229661 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3900\n",
      "learning_rate: 0.00444725\n",
      "Training loss: 0.0750699 , Training acc:  0.978412\n",
      "Test loss: 0.246934 , Test acc:  0.913754\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3920\n",
      "learning_rate: 0.00444725\n",
      "Training loss: 0.0717709 , Training acc:  0.980402\n",
      "Test loss: 0.23959 , Test acc:  0.918828\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3940\n",
      "learning_rate: 0.00444725\n",
      "Training loss: 0.0716133 , Training acc:  0.982292\n",
      "Test loss: 0.236417 , Test acc:  0.918828\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3960\n",
      "learning_rate: 0.00444725\n",
      "Training loss: 0.0646993 , Training acc:  0.984879\n",
      "Test loss: 0.238191 , Test acc:  0.920519\n",
      "--------------------------------------------------------------------------------\n",
      "steps 3980\n",
      "learning_rate: 0.00440278\n",
      "Training loss: 0.0681013 , Training acc:  0.982889\n",
      "Test loss: 0.234893 , Test acc:  0.9177\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4000\n",
      "learning_rate: 0.00440278\n",
      "Training loss: 0.0727816 , Training acc:  0.982591\n",
      "Test loss: 0.245146 , Test acc:  0.914882\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4020\n",
      "learning_rate: 0.00440278\n",
      "Training loss: 0.0729025 , Training acc:  0.979109\n",
      "Test loss: 0.249544 , Test acc:  0.909808\n",
      "--------------------------------------------------------------------------------\n",
      "steps 4040\n",
      "learning_rate: 0.00440278\n",
      "Training loss: 0.066539 , Training acc:  0.982292\n",
      "Test loss: 0.23966 , Test acc:  0.916573\n",
      "--------------------------------------------------------------------------------\n",
      "Optimization Finished!\n",
      "Training Accuracy: 0.982292\n",
      "Test Accuracy: 0.916573\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "RUN_NAME = 'learning_rate_decay_2'\n",
    "writer_train = tf.summary.FileWriter('./log/' + RUN_NAME + '/train', graph=sess.graph)\n",
    "writer_test = tf.summary.FileWriter('./log/' + RUN_NAME + '/test', graph=sess.graph)\n",
    "\n",
    "steps = 0\n",
    "# Keep training until reach max iterations\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train =  X_train[indices], y_train[indices]\n",
    "\n",
    "    for start, end in feed_next_batch(len(X_train), batch_size=batch_size):\n",
    "        # Run optimization op (backprop)\n",
    "        batch_x, batch_y = X_train[start:end], y_train[start:end]\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 20 == 0:\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            train_loss, train_acc, train_lr, summary_train = sess.run([loss, accuracy, learning_rate, summary], \n",
    "                                                   feed_dict={x: X_train, y: y_train, keep_prob: 1.0})\n",
    "            writer_train.add_summary(summary_train, steps)\n",
    "            \n",
    "            print('steps %d' % (steps ))\n",
    "            print('learning_rate:', train_lr)\n",
    "            print (\"Training loss:\", train_loss, ', Training acc: ', train_acc)\n",
    "\n",
    "            val_loss, val_acc, summary_test = sess.run([loss, accuracy, summary], feed_dict={x: X_test, \n",
    "                                                                        y: y_test,\n",
    "                                                                       keep_prob: 1.0})\n",
    "            writer_test.add_summary(summary_test, steps)\n",
    "            print (\"Test loss:\", val_loss, ', Test acc: ', val_acc)\n",
    "            print('-' * 80)\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for all test samples\n",
    "print (\"Training Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_train,\n",
    "                                  y: y_train,\n",
    "                                 keep_prob: 1.0}))\n",
    "print (\"Test Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x: X_test,\n",
    "                                  y: y_test,\n",
    "                                 keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEECAYAAAAPo8LjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1260d1dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess.close()\n",
    "\n",
    "arr5 = np.arange(0, 3600)\n",
    "arr5 = 0.0075 * 0.99 ** (arr5/75)\n",
    "plt.plot(arr5, '--'),\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
