{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# import librosa\n",
    "# from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# import pydot\n",
    "# import graphviz\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13\n",
    "\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Leapmotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('data_ready.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot (integer_encoded):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "y_leap_train = one_hot(train_label)\n",
    "y_leap_test = one_hot(test_label)\n",
    "X_leap_train = train\n",
    "X_leap_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884, 100, 87)\n",
      "(299, 100, 87)\n",
      "(884, 6)\n",
      "(299, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_leap_train.shape)\n",
    "print(X_leap_test.shape)\n",
    "print(y_leap_train.shape)\n",
    "print(y_leap_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('nn_simple_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('nn_simple_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "X_all = features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_all = enc.fit_transform(labels.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/py35deeplearning/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_voice_train, X_voice_test, y_voice_train, y_voice_test = train_test_split(\n",
    "        X_all, y_all, stratify=y_all, train_size=.75, random_state=round(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10633, 1280)\n",
      "(3545, 1280)\n",
      "(10633, 6)\n",
      "(3545, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_voice_train.shape)\n",
    "print(X_voice_test.shape)\n",
    "print(y_voice_train.shape)\n",
    "print(y_voice_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Leapmotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 87\n",
    "timesteps = 100\n",
    "num_classes = 6\n",
    "leap_batch_size = 55\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "def build_leapmotion_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(55, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(55, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(55, return_sequences=True)) # return a single vector of dimension 32\n",
    "    model.add(LSTM(55))\n",
    "    model.add(Dense(55))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Voice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import print_summary, plot_model\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 40, 32\n",
    "\n",
    "X_voice_train = X_voice_train.reshape(X_voice_train.shape[0], img_rows, img_cols, 1)\n",
    "X_voice_test = X_voice_test.reshape(X_voice_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voice_model(input_shape=(img_rows, img_cols, 1)):\n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapmotion_model = build_leapmotion_model()\n",
    "voice_model = build_voice_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/py35deeplearning/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([leapmotion_model, voice_model], mode='concat', concat_axis=-1))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4420/4420 [==============================] - 71s 16ms/step - loss: 1.7048 - acc: 0.2794\n",
      "Epoch 2/20\n",
      "4420/4420 [==============================] - 65s 15ms/step - loss: 1.1417 - acc: 0.4855\n",
      "Epoch 3/20\n",
      "4420/4420 [==============================] - 64s 14ms/step - loss: 0.9208 - acc: 0.5796\n",
      "Epoch 4/20\n",
      "4420/4420 [==============================] - 65s 15ms/step - loss: 0.7708 - acc: 0.6600\n",
      "Epoch 5/20\n",
      "4420/4420 [==============================] - 65s 15ms/step - loss: 0.7085 - acc: 0.6796\n",
      "Epoch 6/20\n",
      "4420/4420 [==============================] - 64s 14ms/step - loss: 0.5842 - acc: 0.7339\n",
      "Epoch 7/20\n",
      "4420/4420 [==============================] - 65s 15ms/step - loss: 0.5310 - acc: 0.7792\n",
      "Epoch 8/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.4544 - acc: 0.8484\n",
      "Epoch 9/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.3386 - acc: 0.9034\n",
      "Epoch 10/20\n",
      "4420/4420 [==============================] - 64s 15ms/step - loss: 0.2610 - acc: 0.9303\n",
      "Epoch 11/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.2426 - acc: 0.9353\n",
      "Epoch 12/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.2178 - acc: 0.9410\n",
      "Epoch 13/20\n",
      "4420/4420 [==============================] - 68s 15ms/step - loss: 0.1864 - acc: 0.9516\n",
      "Epoch 14/20\n",
      "4420/4420 [==============================] - 75s 17ms/step - loss: 0.1499 - acc: 0.9609\n",
      "Epoch 15/20\n",
      "4420/4420 [==============================] - 75s 17ms/step - loss: 0.2157 - acc: 0.9475\n",
      "Epoch 16/20\n",
      "4420/4420 [==============================] - 73s 17ms/step - loss: 0.1463 - acc: 0.9631\n",
      "Epoch 17/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.1600 - acc: 0.9609\n",
      "Epoch 18/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.1295 - acc: 0.9652\n",
      "Epoch 19/20\n",
      "4420/4420 [==============================] - 66s 15ms/step - loss: 0.1527 - acc: 0.9622\n",
      "Epoch 20/20\n",
      "4420/4420 [==============================] - 64s 15ms/step - loss: 0.0972 - acc: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f126a094828>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_leap_sub_train, X_voice_sub_train], y_voice_sub_train,\n",
    "          batch_size=55, shuffle=False,\n",
    "          epochs=20,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voice_sub_train = []\n",
    "y_voice_sub_train = []\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    for i in range(6):\n",
    "        cnt = np.argmax(y_leap_train, axis=1).tolist().count(i)\n",
    "\n",
    "        tmp = list(filter(lambda d: np.argmax(d[1])==i, zip(X_voice_train,y_voice_train)))\n",
    "        shuffle(tmp)\n",
    "\n",
    "        X_voice_sub_train += list(map(lambda d: d[0], tmp[:cnt]))\n",
    "        y_voice_sub_train += list(map(lambda d: d[1], tmp[:cnt]))\n",
    "    \n",
    "\n",
    "X_voice_sub_train = np.array(X_voice_sub_train)\n",
    "y_voice_sub_train = np.array(y_voice_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4420, 40, 32, 1)\n",
      "(4420, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_voice_sub_train.shape)\n",
    "print(y_voice_sub_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_leap_sub_train = np.empty((0, 100, 87))\n",
    "for _ in range(5):\n",
    "    X_leap_sub_train = np.vstack([X_leap_sub_train, X_leap_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 100, 87)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_leap_sub_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voice_sub_test = []\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(6):\n",
    "    cnt = np.argmax(y_leap_test, axis=1).tolist().count(i)\n",
    "\n",
    "    tmp = list(filter(lambda d: d[1]==i, zip(X_voice_test, np.argmax(y_voice_test, axis=1))))\n",
    "\n",
    "    X_voice_sub_test += list(map(lambda d: d[0], tmp[:cnt]))\n",
    "    \n",
    "\n",
    "X_voice_sub_test = np.array(X_voice_sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 40, 32, 1)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_voice_sub_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 4s 12ms/step\n",
      "\n",
      "Test loss: 0.33863449306616616\n",
      "Test accuracy: 0.9331103678929766\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([X_leap_test, X_voice_sub_test], y_leap_test, verbose=1)\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
