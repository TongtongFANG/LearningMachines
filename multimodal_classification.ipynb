{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# import librosa\n",
    "# from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# import pydot\n",
    "# import graphviz\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "dataset_path_prefix = 'dataset'\n",
    "model_path_prefix = 'trained_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Leapmotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_features = np.loadtxt(os.path.join(dataset_path_prefix, 'leap_merge_features.csv'), delimiter=',')\n",
    "leap_labels = np.array(np.loadtxt(os.path.join(dataset_path_prefix, 'leap_merge_labels.csv'), delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 8700)\n",
      "(296, 8700)\n",
      "(887, 6)\n",
      "(296, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitmann/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_leap_train, X_leap_test, y_leap_train, y_leap_test = train_test_split(\n",
    "        leap_features, leap_labels, stratify=leap_labels, train_size=.75, random_state=round(time.time()))\n",
    "\n",
    "print(X_leap_train.shape)\n",
    "print(X_leap_test.shape)\n",
    "print(y_leap_train.shape)\n",
    "print(y_leap_test.shape)\n",
    "\n",
    "num_rows, num_cols = 100, 87\n",
    "\n",
    "X_leap_train = X_leap_train.reshape(X_leap_train.shape[0], num_rows, num_cols)\n",
    "X_leap_test = X_leap_test.reshape(X_leap_test.shape[0], num_rows, num_cols)\n",
    "\n",
    "print(\"After reshaping\")\n",
    "print(X_leap_train.shape)\n",
    "print(X_leap_test.shape)\n",
    "print(y_leap_train.shape)\n",
    "print(y_leap_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('voice_merge_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('voice_merge_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_all = enc.fit_transform(labels.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitmann/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_voice_train, X_voice_test, y_voice_train, y_voice_test = train_test_split(\n",
    "        X_all, y_all, stratify=y_all, train_size=.75, random_state=round(time.time()))\n",
    "\n",
    "print(X_voice_train.shape)\n",
    "print(X_voice_test.shape)\n",
    "print(y_voice_train.shape)\n",
    "print(y_voice_test.shape)\n",
    "\n",
    "num_rows, num_cols = 100, 87\n",
    "\n",
    "X_leap_train = X_leap_train.reshape(X_leap_train.shape[0], num_rows, num_cols)\n",
    "X_leap_test = X_leap_test.reshape(X_leap_test.shape[0], num_rows, num_cols)\n",
    "\n",
    "print(\"After reshaping\")\n",
    "print(X_voice_train.shape)\n",
    "print(X_voice_test.shape)\n",
    "print(y_voice_train.shape)\n",
    "print(y_voice_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from models import ResearchModels\n",
    "from data import DataSet\n",
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the classes order\n",
      "['right', 'left', 'up', 'down', 'start', 'no']\n",
      "Loading 984 samples into memory for training.\n",
      "Loading 389 samples into memory for testing.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mlp_merge'\n",
    "saved_model = None  # None or weights file\n",
    "class_limit = 6  # int, can be 1-101 or None\n",
    "seq_length = 40\n",
    "load_to_memory = True  # pre-load the sequences into memory\n",
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "# Chose images or features and image shape based on network.\n",
    "if model_name in ['conv_3d', 'c3d', 'lrcn']:\n",
    "    data_type = 'images'\n",
    "    image_shape = (80, 80, 3)\n",
    "elif model_name in ['lstm', 'mlp', 'mlp_merge']:\n",
    "    data_type = 'features'\n",
    "    image_shape = None\n",
    "else:\n",
    "    raise ValueError(\"Invalid model. See train.py for options.\")\n",
    "\n",
    "    \n",
    "# Helper: Save the model.\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('data', 'checkpoints', model_name + '-' + data_type + \\\n",
    "        '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('data', 'logs', model_name))\n",
    "\n",
    "# Helper: Stop when we stop learning.\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('data', 'logs', model_name + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "    \n",
    "\n",
    "# Get the data and process it.\n",
    "if image_shape is None:\n",
    "    data = DataSet(\n",
    "        seq_length=seq_length,\n",
    "        class_limit=class_limit\n",
    "    )\n",
    "else:\n",
    "    data = DataSet(\n",
    "        seq_length=seq_length,\n",
    "        class_limit=class_limit,\n",
    "        image_shape=image_shape\n",
    "    )\n",
    "\n",
    "# Get samples per epoch.\n",
    "# Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
    "\n",
    "if load_to_memory:\n",
    "    # Get data.\n",
    "    X_video_train, y_video_train = data.get_all_sequences_in_memory('train', data_type)\n",
    "    X_video_test, y_video_test = data.get_all_sequences_in_memory('test', data_type)\n",
    "else:\n",
    "    # Get generators.\n",
    "    generator = data.frame_generator(batch_size, 'train', data_type)\n",
    "    val_generator = data.frame_generator(batch_size, 'test', data_type)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 40, 2048)\n",
      "(344, 40, 2048)\n",
      "(1029, 6)\n",
      "(344, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_video_train.shape)\n",
    "print(X_video_test.shape)\n",
    "print(y_video_train.shape)\n",
    "print(y_video_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.55454561e-01,  9.82281327e-01,  1.04676999e-01, -7.39377439e-01,\n",
       "       -4.54269983e-02, -6.71756923e-01, -2.26899643e+01,  1.49956421e+02,\n",
       "        1.20354774e+02, -4.77434736e+01,  9.60827789e+01, -1.71007025e+02,\n",
       "       -3.05654907e+01,  1.00049690e+02,  1.24440170e+02, -3.05654907e+01,\n",
       "        1.00049690e+02,  1.24440170e+02, -5.67678566e+01,  1.27429832e+02,\n",
       "        1.40550293e+02, -6.92803574e+01,  1.51950836e+02,  1.34779343e+02,\n",
       "       -7.44361649e+01,  1.68014343e+02,  1.25391136e+02, -1.56523962e+01,\n",
       "        1.10663216e+02,  1.31320755e+02, -3.81087227e+01,  1.66541580e+02,\n",
       "        1.38856659e+02, -5.16770096e+01,  1.99280258e+02,  1.38388321e+02,\n",
       "       -6.04333153e+01,  2.17131866e+02,  1.36895081e+02, -6.72023621e+01,\n",
       "        2.29374603e+02,  1.35174576e+02, -8.63309574e+00,  1.14827522e+02,\n",
       "        1.25273323e+02, -2.49031429e+01,  1.70015961e+02,  1.26739426e+02,\n",
       "       -3.76661148e+01,  2.06400635e+02,  1.17025009e+02, -4.74663773e+01,\n",
       "        2.26412659e+02,  1.09691063e+02, -5.49904060e+01,  2.38763748e+02,\n",
       "        1.04108192e+02, -2.81986594e+00,  1.17456322e+02,  1.17398636e+02,\n",
       "       -1.22649336e+01,  1.68066788e+02,  1.12960838e+02, -2.52019291e+01,\n",
       "        2.01372025e+02,  1.03907982e+02, -3.57941666e+01,  2.20329453e+02,\n",
       "        9.67892990e+01, -4.41149406e+01,  2.32089203e+02,  9.13077545e+01,\n",
       "       -6.56550050e-01,  1.17062943e+02,  1.05865387e+02, -3.41231918e+00,\n",
       "        1.64195175e+02,  9.81737137e+01, -7.72871733e+00,  1.91742447e+02,\n",
       "        8.96069412e+01, -1.32706003e+01,  2.05742554e+02,  8.38080902e+01,\n",
       "       -2.01594620e+01,  2.16815720e+02,  7.81401978e+01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_leap_test[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Leapmotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 87\n",
    "timesteps = 100\n",
    "num_classes = 6\n",
    "leap_batch_size = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import print_summary, plot_model\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 40, 32\n",
    "\n",
    "X_voice_train = X_voice_train.reshape(X_voice_train.shape[0], img_rows, img_cols, 1)\n",
    "X_voice_test = X_voice_test.reshape(X_voice_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representation_layer(model_name, trainable=False, new_model=False):\n",
    "    \n",
    "    model = load_model(os.path.join(model_path_prefix, model_name))\n",
    "    if new_model:\n",
    "        model = Sequential.from_config(model.get_config())\n",
    "    else:\n",
    "        model.trainable = trainable\n",
    "    \n",
    "    return model, model.layers[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_model, leapmotion_representation_output = extract_representation_layer(\"leap_model.h5\", trainable=True, new_model=False)\n",
    "voice_model, voice_representation_output = extract_representation_layer(\"voice_model.h5\", trainable=True, new_model=False)\n",
    "video_model, video_representation_output = extract_representation_layer(\"video_model.h5\", trainable=True, new_model=False)\n",
    "# leap_model = load_model(\"leap_model.h5\")\n",
    "# voice_model = load_model(\"voice_model.h5\")\n",
    "# video_model = ResearchModels(len(data.classes), model_name, seq_length, saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/py35deeplearning/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge, concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "\n",
    "# merge_layer = concatenate([leapmotion_representation_output, voice_representation_output])\n",
    "# dense_layer = Dense(64, activation='relu', name=\"merge_dense\")(merge_layer)\n",
    "# dense_layer = Dropout(0.5, name=\"merge_dropout\")(dense_layer)\n",
    "# output_layer = Dense(num_classes, activation='softmax', name=\"merge_output\")(dense_layer)\n",
    "\n",
    "# merge_model = Model(inputs=[leap_model.input, voice_model.input], outputs=output_layer)\n",
    "\n",
    "merge_model = Sequential()\n",
    "merge_model.add(Merge([video_representation_output, voice_representation_output], mode='concat', concat_axis=-1))\n",
    "\n",
    "merge_model.add(Dense(128, activation='relu', name=\"merge_dense_1\"))\n",
    "merge_model.add(Dropout(0.8, name=\"merge_dropout_1\"))\n",
    "# merge_model.add(Dense(128, activation='relu', name=\"merge_dense_2\"))\n",
    "# merge_model.add(Dropout(0.5, name=\"merge_dropout_2\"))\n",
    "merge_model.add(Dense(num_classes, activation='softmax', name=\"merge_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "merge_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4405 samples, validate on 881 samples\n",
      "Epoch 1/10\n",
      "4384/4405 [============================>.] - ETA: 0s - loss: 13.0269 - acc: 0.1914"
     ]
    }
   ],
   "source": [
    "merge_model.fit([X_merge_video_train, X_merge_voice_train],\\\n",
    "          y_merge_train,\n",
    "          batch_size=32, shuffle=False,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([X_merge_video_val, X_merge_voice_val], y_merge_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5920/5920 [==============================] - 14s 2ms/step\n",
      "\n",
      "Test loss: 5.43571733797\n",
      "Test accuracy: 0.222804054054\n"
     ]
    }
   ],
   "source": [
    "score = merge_model.evaluate([X_merge_leap_test, X_merge_voice_test], y_merge_test, verbose=1)\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_data(X_voice, y_voice,\\\n",
    "                        X_leap, y_leap,\\\n",
    "                        X_video, y_video,\\\n",
    "                       augment_index=1, ):\n",
    "\n",
    "    X_merge_voice = []\n",
    "    X_merge_leap = []\n",
    "    X_merge_video = []\n",
    "    y_merge = []\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        voice_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_voice,y_voice))))\n",
    "        leap_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_leap,y_leap))))\n",
    "        video_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_video,y_video))))\n",
    "        \n",
    "        for _ in range(augment_index):\n",
    "            \n",
    "            shuffle(voice_tmp)\n",
    "            shuffle(leap_tmp)\n",
    "            shuffle(video_tmp)\n",
    "\n",
    "            for tuple_3 in zip(voice_tmp, leap_tmp, video_tmp):\n",
    "                X_merge_voice.append(tuple_3[0])\n",
    "                X_merge_leap.append(tuple_3[1])\n",
    "                X_merge_video.append(tuple_3[2])\n",
    "\n",
    "                y_merge.append(i)\n",
    "                \n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    return np.array(X_merge_voice), np.array(X_merge_leap), np.array(X_merge_video), ohe.fit_transform(np.array(y_merge).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merge_voice_train,\\\n",
    "X_merge_leap_train,\\\n",
    "X_merge_video_train,\\\n",
    "y_merge_train = generate_merge_data(X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    X_leap_train,\\\n",
    "                                    y_leap_train,\\\n",
    "                                    X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    augment_index=5)\n",
    "\n",
    "X_merge_voice_val,\\\n",
    "X_merge_leap_val,\\\n",
    "X_merge_video_val,\\\n",
    "y_merge_val = generate_merge_data(X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    X_leap_train,\\\n",
    "                                    y_leap_train,\\\n",
    "                                    X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    augment_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4435, 40, 32, 1)\n",
      "(4435, 100, 87)\n",
      "(4435, 40, 32, 1)\n",
      "(4435, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_train.shape))\n",
    "print((X_merge_leap_train.shape))\n",
    "print((X_merge_video_train.shape))\n",
    "print((y_merge_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 40, 32, 1)\n",
      "(887, 100, 87)\n",
      "(887, 40, 32, 1)\n",
      "(887, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_val.shape))\n",
    "print((X_merge_leap_val.shape))\n",
    "print((X_merge_video_val.shape))\n",
    "print((y_merge_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merge_voice_test,\\\n",
    "X_merge_leap_test,\\\n",
    "X_merge_video_test,\\\n",
    "y_merge_test = generate_merge_data(X_voice_test,\\\n",
    "                                    y_voice_test,\\\n",
    "                                    X_leap_test,\\\n",
    "                                    y_leap_test,\\\n",
    "                                    X_leap_test,\\\n",
    "                                    y_leap_test,\\\n",
    "                                    augment_index=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5920, 40, 32, 1)\n",
      "(5920, 100, 87)\n",
      "(5920, 100, 87)\n",
      "(5920, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_test.shape))\n",
    "print((X_merge_leap_test.shape))\n",
    "print((X_merge_video_test.shape))\n",
    "print((y_merge_test.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
