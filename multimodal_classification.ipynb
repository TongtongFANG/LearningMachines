{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# import librosa\n",
    "# from librosa import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# import pydot\n",
    "# import graphviz\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Leapmotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_features = np.loadtxt('leap_merge_features.csv', delimiter=',')\n",
    "leap_labels = np.array(np.loadtxt('leap_merge_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 8700)\n",
      "(296, 8700)\n",
      "(887, 6)\n",
      "(296, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitmann/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_leap_train, X_leap_test, y_leap_train, y_leap_test = train_test_split(\n",
    "        leap_features, leap_labels, stratify=leap_labels, train_size=.75, random_state=round(time.time()))\n",
    "\n",
    "print(X_leap_train.shape)\n",
    "print(X_leap_test.shape)\n",
    "print(y_leap_train.shape)\n",
    "print(y_leap_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('voice_merge_features.csv', delimiter=',')\n",
    "labels = np.array(np.loadtxt('voice_merge_labels.csv', delimiter=','), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_all = enc.fit_transform(labels.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitmann/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_voice_train, X_voice_test, y_voice_train, y_voice_test = train_test_split(\n",
    "        X_all, y_all, stratify=y_all, train_size=.75, random_state=round(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10633, 1280)\n",
      "(3545, 1280)\n",
      "(10633, 6)\n",
      "(3545, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_voice_train.shape)\n",
    "print(X_voice_test.shape)\n",
    "print(y_voice_train.shape)\n",
    "print(y_voice_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-12d7b025f486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResearchModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from models import ResearchModels\n",
    "from data import DataSet\n",
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the classes order\n",
      "['right', 'left', 'up', 'down', 'start', 'no']\n",
      "Loading 984 samples into memory for training.\n",
      "Loading 389 samples into memory for testing.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mlp_merge'\n",
    "saved_model = None  # None or weights file\n",
    "class_limit = 6  # int, can be 1-101 or None\n",
    "seq_length = 40\n",
    "load_to_memory = True  # pre-load the sequences into memory\n",
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "# Chose images or features and image shape based on network.\n",
    "if model_name in ['conv_3d', 'c3d', 'lrcn']:\n",
    "    data_type = 'images'\n",
    "    image_shape = (80, 80, 3)\n",
    "elif model_name in ['lstm', 'mlp', 'mlp_merge']:\n",
    "    data_type = 'features'\n",
    "    image_shape = None\n",
    "else:\n",
    "    raise ValueError(\"Invalid model. See train.py for options.\")\n",
    "\n",
    "    \n",
    "# Helper: Save the model.\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('data', 'checkpoints', model_name + '-' + data_type + \\\n",
    "        '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('data', 'logs', model_name))\n",
    "\n",
    "# Helper: Stop when we stop learning.\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('data', 'logs', model_name + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "    \n",
    "\n",
    "# Get the data and process it.\n",
    "if image_shape is None:\n",
    "    data = DataSet(\n",
    "        seq_length=seq_length,\n",
    "        class_limit=class_limit\n",
    "    )\n",
    "else:\n",
    "    data = DataSet(\n",
    "        seq_length=seq_length,\n",
    "        class_limit=class_limit,\n",
    "        image_shape=image_shape\n",
    "    )\n",
    "\n",
    "# Get samples per epoch.\n",
    "# Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
    "\n",
    "if load_to_memory:\n",
    "    # Get data.\n",
    "    X_video_train, y_video_train = data.get_all_sequences_in_memory('train', data_type)\n",
    "    X_video_test, y_video_test = data.get_all_sequences_in_memory('test', data_type)\n",
    "else:\n",
    "    # Get generators.\n",
    "    generator = data.frame_generator(batch_size, 'train', data_type)\n",
    "    val_generator = data.frame_generator(batch_size, 'test', data_type)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(984, 40, 2048)\n",
      "(389, 40, 2048)\n",
      "(984, 6)\n",
      "(389, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_video_train.shape)\n",
    "print(X_video_test.shape)\n",
    "print(y_video_train.shape)\n",
    "print(y_video_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Leapmotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 87\n",
    "timesteps = 100\n",
    "num_classes = 6\n",
    "leap_batch_size = 55\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "def build_leapmotion_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(55, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    # model.add(LSTM(55, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    # model.add(LSTM(55, return_sequences=True)) # return a single vector of dimension 32\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(55))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(55))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_cols = 100, 87\n",
    "\n",
    "X_leap_train = X_leap_train.reshape(X_leap_train.shape[0], num_rows, num_cols)\n",
    "X_leap_test = X_leap_test.reshape(X_leap_test.shape[0], num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Voice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import print_summary, plot_model\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 40, 32\n",
    "\n",
    "X_voice_train = X_voice_train.reshape(X_voice_train.shape[0], img_rows, img_cols, 1)\n",
    "X_voice_test = X_voice_test.reshape(X_voice_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voice_model(input_shape=(img_rows, img_cols, 1)):\n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapmotion_model = build_leapmotion_model()\n",
    "voice_model = build_voice_model()\n",
    "# leap_model = load_model(\"leap_model.h5\")\n",
    "# voice_model = load_model(\"voice_model.h5\")\n",
    "# video_model = ResearchModels(len(data.classes), model_name, seq_length, saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitmann/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([leapmotion_model, voice_model], mode='concat', concat_axis=-1))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4435 samples, validate on 887 samples\n",
      "Epoch 1/20\n",
      "4435/4435 [==============================] - 25s 6ms/step - loss: 1.8678 - acc: 0.3563 - val_loss: 3.9482 - val_acc: 0.2311\n",
      "Epoch 2/20\n",
      "4435/4435 [==============================] - 21s 5ms/step - loss: 1.5172 - acc: 0.5098 - val_loss: 4.8243 - val_acc: 0.3145\n",
      "Epoch 3/20\n",
      "4435/4435 [==============================] - 22s 5ms/step - loss: 1.1880 - acc: 0.6489 - val_loss: 5.1643 - val_acc: 0.3405\n",
      "Epoch 4/20\n",
      "4435/4435 [==============================] - 22s 5ms/step - loss: 0.9993 - acc: 0.6970 - val_loss: 4.9730 - val_acc: 0.3574\n",
      "Epoch 5/20\n",
      "4435/4435 [==============================] - 23s 5ms/step - loss: 0.9210 - acc: 0.7333 - val_loss: 5.1930 - val_acc: 0.3439\n",
      "Epoch 6/20\n",
      "4435/4435 [==============================] - 23s 5ms/step - loss: 0.9030 - acc: 0.7353 - val_loss: 5.6976 - val_acc: 0.3179\n",
      "Epoch 7/20\n",
      "4435/4435 [==============================] - 23s 5ms/step - loss: 0.7770 - acc: 0.7844 - val_loss: 6.5767 - val_acc: 0.3179\n",
      "Epoch 8/20\n",
      "2112/4435 [=============>................] - ETA: 12s - loss: 0.7927 - acc: 0.7713"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ad8ae6626c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=([X_merge_leap_val, X_merge_voice_val], y_merge_val))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([X_merge_leap_train, X_merge_voice_train],\\\n",
    "          y_merge_train,\n",
    "          batch_size=64, shuffle=False,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=([X_merge_leap_val, X_merge_voice_val], y_merge_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5980/5980 [==============================] - 24s 4ms/step\n",
      "\n",
      "Test loss: 0.23723840368724736\n",
      "Test accuracy: 0.9260869564021311\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([X_merge_leap_test, X_merge_voice_test], y_merge_test, verbose=1)\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_data(X_voice, y_voice,\\\n",
    "                        X_leap, y_leap,\\\n",
    "                        X_video, y_video,\\\n",
    "                       augment_index=1, ):\n",
    "\n",
    "    X_merge_voice = []\n",
    "    X_merge_leap = []\n",
    "    X_merge_video = []\n",
    "    y_merge = []\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        voice_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_voice,y_voice))))\n",
    "        leap_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_leap,y_leap))))\n",
    "        video_tmp = list(map(lambda d:d[0], filter(lambda d: np.argmax(d[1])==i, zip(X_video,y_video))))\n",
    "        \n",
    "        for _ in range(augment_index):\n",
    "            \n",
    "            shuffle(voice_tmp)\n",
    "            shuffle(leap_tmp)\n",
    "            shuffle(video_tmp)\n",
    "\n",
    "            for tuple_3 in zip(voice_tmp, leap_tmp, video_tmp):\n",
    "                X_merge_voice.append(tuple_3[0])\n",
    "                X_merge_leap.append(tuple_3[1])\n",
    "                X_merge_video.append(tuple_3[2])\n",
    "\n",
    "                y_merge.append(i)\n",
    "                \n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    return np.array(X_merge_voice), np.array(X_merge_leap), np.array(X_merge_video), ohe.fit_transform(np.array(y_merge).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merge_voice_train,\\\n",
    "X_merge_leap_train,\\\n",
    "X_merge_video_train,\\\n",
    "y_merge_train = generate_merge_data(X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    X_leap_train,\\\n",
    "                                    y_leap_train,\\\n",
    "                                    X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    augment_index=5)\n",
    "\n",
    "X_merge_voice_val,\\\n",
    "X_merge_leap_val,\\\n",
    "X_merge_video_val,\\\n",
    "y_merge_val = generate_merge_data(X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    X_leap_train,\\\n",
    "                                    y_leap_train,\\\n",
    "                                    X_voice_train,\\\n",
    "                                    y_voice_train,\\\n",
    "                                    augment_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4435, 40, 32, 1)\n",
      "(4435, 100, 87)\n",
      "(4435, 40, 32, 1)\n",
      "(4435, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_train.shape))\n",
    "print((X_merge_leap_train.shape))\n",
    "print((X_merge_video_train.shape))\n",
    "print((y_merge_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 40, 32, 1)\n",
      "(887, 100, 87)\n",
      "(887, 40, 32, 1)\n",
      "(887, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_val.shape))\n",
    "print((X_merge_leap_val.shape))\n",
    "print((X_merge_video_val.shape))\n",
    "print((y_merge_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merge_voice_test,\\\n",
    "X_merge_leap_test,\\\n",
    "X_merge_video_test,\\\n",
    "y_merge_test = generate_merge_data(X_voice_test,\\\n",
    "                                    y_voice_test,\\\n",
    "                                    X_leap_test,\\\n",
    "                                    y_leap_test,\\\n",
    "                                    X_leap_test,\\\n",
    "                                    y_leap_test,\\\n",
    "                                    augment_index=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5920, 40, 32, 1)\n",
      "(5920, 100, 87)\n",
      "(5920, 100, 87)\n",
      "(5920, 6)\n"
     ]
    }
   ],
   "source": [
    "print((X_merge_voice_test.shape))\n",
    "print((X_merge_leap_test.shape))\n",
    "print((X_merge_video_test.shape))\n",
    "print((y_merge_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04563421, 0.05056863, 0.05538939, 0.07386345, 0.01360721,\n",
       "       0.02085993, 0.0332799 , 0.01394629, 0.01090135, 0.00746803,\n",
       "       0.01606545, 0.00902444, 0.01460904, 0.01598709, 0.00903382,\n",
       "       0.0153631 , 0.00978239, 0.01720455, 0.01530915, 0.01020595,\n",
       "       0.01140743, 0.00635595, 0.00841652, 0.0112095 , 0.01028466,\n",
       "       0.03030672, 0.0327414 , 0.02451475, 0.02913259, 0.01686394,\n",
       "       0.01783792, 0.00886245, 0.0191673 , 0.02494109, 0.01072723,\n",
       "       0.00994156, 0.01186314, 0.01294528, 0.01531062, 0.01087279],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_video_train[0][:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
